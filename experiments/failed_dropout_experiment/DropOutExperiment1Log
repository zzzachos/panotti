Last login: Sun Nov 17 17:11:46 on ttys000
(base) DNa1c06a0:~ evangelie$ cd ~/Documents/GitHub/panotti/
(base) DNa1c06a0:panotti evangelie$ source activate tensorflow
(tensorflow) DNa1c06a0:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1045_IOM_ANG_XX.wa Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1068_DFA_ANG_XX. Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1004_IEO_ANG_MD. Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1009_TIE_ANG_XX. Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1083_MTI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1047_TIE_DIS_XX.wa Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1044_IWW_DIS_XX. Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1090_IEO_DIS_LO. Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1067_IWW_DIS_XX. Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1075_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1069_IOM_FEA_XX.wa Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1078_ITH_FEA_XX. Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1012_ITH_FEA_XX. Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1085_IEO_FEA_MD. Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1047_ITH_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1089_WSI_HAP_XX.wa Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1050_IWL_HAP_XX. Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1004_DFA_HAP_XX. Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1041_TSI_HAP_XX. Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1037_TIE_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1060_TAI_NEU_XX.wa Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1057_IWW_NEU_XX. Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1059_IWL_NEU_XX. Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1017_TAI_NEU_XX. Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1057_TIE_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1038_IOM_SAD_XX.wa Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1047_MTI_SAD_XX. Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1015_IOM_SAD_XX. Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1072_TSI_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-17 17:30:47.235640: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-17 17:30:47.235939: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_1 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_2 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_3 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_4 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_5 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 163s 96ms/step - loss: 2.8534 - accuracy: 0.2549 - val_loss: 2.0550 - val_accuracy: 0.2547

Epoch 00001: val_loss improved from inf to 2.05505, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0858 - accuracy: 0.2808 - val_loss: 1.7376 - val_accuracy: 0.2028

Epoch 00002: val_loss improved from 2.05505 to 1.73758, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9668 - accuracy: 0.3050 - val_loss: 1.8363 - val_accuracy: 0.3349

Epoch 00003: val_loss did not improve
Epoch 4/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.8923 - accuracy: 0.3204 - val_loss: 2.6483 - val_accuracy: 0.2382

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.8288 - accuracy: 0.3204 - val_loss: 1.4828 - val_accuracy: 0.4245

Epoch 00005: val_loss improved from 1.73758 to 1.48279, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.7391 - accuracy: 0.3457 - val_loss: 1.6677 - val_accuracy: 0.3491

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.7090 - accuracy: 0.3410 - val_loss: 1.5171 - val_accuracy: 0.3939

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.6375 - accuracy: 0.3729 - val_loss: 1.5187 - val_accuracy: 0.4104

Epoch 00008: val_loss did not improve
Epoch 9/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.6019 - accuracy: 0.3746 - val_loss: 1.4994 - val_accuracy: 0.4198

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.5829 - accuracy: 0.3622 - val_loss: 1.5071 - val_accuracy: 0.4127

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.5310 - accuracy: 0.3876 - val_loss: 1.6653 - val_accuracy: 0.3939

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.5110 - accuracy: 0.3894 - val_loss: 1.4496 - val_accuracy: 0.4245

Epoch 00012: val_loss improved from 1.48279 to 1.44956, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 13/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.4871 - accuracy: 0.4018 - val_loss: 1.4630 - val_accuracy: 0.4033

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4886 - accuracy: 0.4018 - val_loss: 1.4801 - val_accuracy: 0.4080

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.4460 - accuracy: 0.4301 - val_loss: 1.3959 - val_accuracy: 0.4575

Epoch 00015: val_loss improved from 1.44956 to 1.39592, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4096 - accuracy: 0.4419 - val_loss: 1.4449 - val_accuracy: 0.4340

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4216 - accuracy: 0.4319 - val_loss: 1.5136 - val_accuracy: 0.4599

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3861 - accuracy: 0.4431 - val_loss: 1.3952 - val_accuracy: 0.4363

Epoch 00018: val_loss improved from 1.39592 to 1.39522, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 19/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3730 - accuracy: 0.4531 - val_loss: 1.4694 - val_accuracy: 0.4222

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.3867 - accuracy: 0.4454 - val_loss: 1.4124 - val_accuracy: 0.4505

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3434 - accuracy: 0.4726 - val_loss: 1.3989 - val_accuracy: 0.4505

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.3269 - accuracy: 0.4673 - val_loss: 1.4024 - val_accuracy: 0.4481

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3263 - accuracy: 0.4814 - val_loss: 1.3609 - val_accuracy: 0.4505

Epoch 00023: val_loss improved from 1.39522 to 1.36085, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 24/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.3039 - accuracy: 0.4791 - val_loss: 1.4427 - val_accuracy: 0.4151

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 166s 98ms/step - loss: 1.3110 - accuracy: 0.4779 - val_loss: 1.4700 - val_accuracy: 0.4292

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.2870 - accuracy: 0.4897 - val_loss: 1.3802 - val_accuracy: 0.4788

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.2669 - accuracy: 0.5115 - val_loss: 1.5054 - val_accuracy: 0.3986

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 169s 99ms/step - loss: 1.2805 - accuracy: 0.4861 - val_loss: 1.4558 - val_accuracy: 0.4292

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.2621 - accuracy: 0.5174 - val_loss: 1.3965 - val_accuracy: 0.4387

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 169s 99ms/step - loss: 1.2669 - accuracy: 0.4997 - val_loss: 1.3492 - val_accuracy: 0.4858

Epoch 00030: val_loss improved from 1.36085 to 1.34918, saving model to convdropout0.5densedropout0.6weights.hdf5
Epoch 31/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.2303 - accuracy: 0.5162 - val_loss: 1.4035 - val_accuracy: 0.4410

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.2378 - accuracy: 0.5174 - val_loss: 1.4318 - val_accuracy: 0.4104

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.2246 - accuracy: 0.5298 - val_loss: 1.4029 - val_accuracy: 0.4599

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 169s 99ms/step - loss: 1.2119 - accuracy: 0.5280 - val_loss: 1.3613 - val_accuracy: 0.4670

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.2046 - accuracy: 0.5434 - val_loss: 1.4011 - val_accuracy: 0.4646

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.1919 - accuracy: 0.5392 - val_loss: 1.4626 - val_accuracy: 0.4269

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.1620 - accuracy: 0.5504 - val_loss: 1.4595 - val_accuracy: 0.3986

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1806 - accuracy: 0.5327 - val_loss: 1.4036 - val_accuracy: 0.4552

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.1684 - accuracy: 0.5381 - val_loss: 1.5059 - val_accuracy: 0.4222

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1581 - accuracy: 0.5563 - val_loss: 1.4012 - val_accuracy: 0.4481

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.1557 - accuracy: 0.5404 - val_loss: 1.3962 - val_accuracy: 0.4316

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1457 - accuracy: 0.5522 - val_loss: 1.4268 - val_accuracy: 0.4269

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1263 - accuracy: 0.5676 - val_loss: 1.4456 - val_accuracy: 0.4222

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1330 - accuracy: 0.5658 - val_loss: 1.4435 - val_accuracy: 0.4269

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 170s 101ms/step - loss: 1.1422 - accuracy: 0.5587 - val_loss: 1.4078 - val_accuracy: 0.4481

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1218 - accuracy: 0.5729 - val_loss: 1.5065 - val_accuracy: 0.4387

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.0976 - accuracy: 0.5687 - val_loss: 1.4014 - val_accuracy: 0.4458

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.1161 - accuracy: 0.5617 - val_loss: 1.3931 - val_accuracy: 0.4316

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.0774 - accuracy: 0.5829 - val_loss: 1.4610 - val_accuracy: 0.3939

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 171s 101ms/step - loss: 1.0929 - accuracy: 0.5782 - val_loss: 1.4125 - val_accuracy: 0.4623

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1050_ITS_ANG Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1089_WSI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1058_TAI_DIS Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1088_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1027_IOM_FEA Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1033_IWW_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1042_TIE_HAP Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1059_DFA_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1048_TSI_NEU Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1026_TIE_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1046_IWW_SAD Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1089_ITH_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3299045249035484
Dev set accuracy: 0.5037593841552734
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1077_IEO_ANG_MD.wa Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1041_IEO_ANG_MD. Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1024_IEO_ANG_HI. Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1090_TSI_ANG_XX. Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1069_ITS_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1004_IEO_DIS_LO.wa Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1089_ITH_DIS_XX. Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1035_TAI_DIS_XX. Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1022_ITH_DIS_XX. Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1084_TIE_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1038_TSI_FEA_XX.wa Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1086_TIE_FEA_XX. Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1080_TSI_FEA_XX. Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1008_TAI_FEA_XX. Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1012_ITH_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1067_IOM_HAP_XX.wa Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1047_ITH_HAP_XX. Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1055_DFA_HAP_XX. Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1048_DFA_HAP_XX. Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1066_DFA_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1046_ITH_NEU_XX.wa Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1021_IEO_NEU_XX. Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1080_IWW_NEU_XX. Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1067_MTI_NEU_XX. Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1054_TSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1037_DFA_SAD_XX.wa Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1059_IEO_SAD_LO. Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1087_ITS_SAD_XX. Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1048_WSI_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_6 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_7 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_8 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_9 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_10 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 172s 101ms/step - loss: 2.7207 - accuracy: 0.2484 - val_loss: 2.4397 - val_accuracy: 0.1745

Epoch 00001: val_loss improved from inf to 2.43973, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 173s 102ms/step - loss: 2.1786 - accuracy: 0.2631 - val_loss: 2.2642 - val_accuracy: 0.2689

Epoch 00002: val_loss improved from 2.43973 to 2.26418, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.9748 - accuracy: 0.3044 - val_loss: 1.5915 - val_accuracy: 0.3632

Epoch 00003: val_loss improved from 2.26418 to 1.59147, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.8850 - accuracy: 0.3174 - val_loss: 1.7446 - val_accuracy: 0.2642

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.7991 - accuracy: 0.3310 - val_loss: 1.5800 - val_accuracy: 0.3208

Epoch 00005: val_loss improved from 1.59147 to 1.57996, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.7168 - accuracy: 0.3322 - val_loss: 1.6089 - val_accuracy: 0.2877

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.6465 - accuracy: 0.3652 - val_loss: 1.5928 - val_accuracy: 0.3467

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.5983 - accuracy: 0.3864 - val_loss: 1.5605 - val_accuracy: 0.3420

Epoch 00008: val_loss improved from 1.57996 to 1.56048, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.5720 - accuracy: 0.3853 - val_loss: 1.5559 - val_accuracy: 0.3962

Epoch 00009: val_loss improved from 1.56048 to 1.55595, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 171s 101ms/step - loss: 1.5880 - accuracy: 0.3835 - val_loss: 1.4558 - val_accuracy: 0.4009

Epoch 00010: val_loss improved from 1.55595 to 1.45579, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.5132 - accuracy: 0.4035 - val_loss: 1.4114 - val_accuracy: 0.3939

Epoch 00011: val_loss improved from 1.45579 to 1.41135, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 12/50
1695/1695 [==============================] - 208s 122ms/step - loss: 1.5143 - accuracy: 0.3923 - val_loss: 1.3978 - val_accuracy: 0.4387

Epoch 00012: val_loss improved from 1.41135 to 1.39781, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 13/50
1695/1695 [==============================] - 231s 136ms/step - loss: 1.4694 - accuracy: 0.4112 - val_loss: 1.5158 - val_accuracy: 0.3962

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 221s 130ms/step - loss: 1.4894 - accuracy: 0.3888 - val_loss: 1.5167 - val_accuracy: 0.3443

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 189s 111ms/step - loss: 1.4484 - accuracy: 0.4224 - val_loss: 1.4362 - val_accuracy: 0.3962

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 210s 124ms/step - loss: 1.4479 - accuracy: 0.4165 - val_loss: 1.4228 - val_accuracy: 0.4363

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 190s 112ms/step - loss: 1.4476 - accuracy: 0.4183 - val_loss: 1.4057 - val_accuracy: 0.4269

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 193s 114ms/step - loss: 1.4070 - accuracy: 0.4407 - val_loss: 1.4159 - val_accuracy: 0.4434

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 229s 135ms/step - loss: 1.3993 - accuracy: 0.4513 - val_loss: 1.3888 - val_accuracy: 0.4552

Epoch 00019: val_loss improved from 1.39781 to 1.38879, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 20/50
1695/1695 [==============================] - 199s 118ms/step - loss: 1.3932 - accuracy: 0.4484 - val_loss: 1.3834 - val_accuracy: 0.4387

Epoch 00020: val_loss improved from 1.38879 to 1.38335, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 21/50
1695/1695 [==============================] - 188s 111ms/step - loss: 1.3550 - accuracy: 0.4684 - val_loss: 1.4233 - val_accuracy: 0.4599

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 227s 134ms/step - loss: 1.3653 - accuracy: 0.4596 - val_loss: 1.3990 - val_accuracy: 0.3986

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 239s 141ms/step - loss: 1.3411 - accuracy: 0.4720 - val_loss: 1.3797 - val_accuracy: 0.4505

Epoch 00023: val_loss improved from 1.38335 to 1.37968, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 24/50
1695/1695 [==============================] - 211s 125ms/step - loss: 1.3281 - accuracy: 0.4702 - val_loss: 1.4054 - val_accuracy: 0.4057

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 213s 126ms/step - loss: 1.3108 - accuracy: 0.4914 - val_loss: 1.3452 - val_accuracy: 0.4575

Epoch 00025: val_loss improved from 1.37968 to 1.34517, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 26/50
1695/1695 [==============================] - 249s 147ms/step - loss: 1.3161 - accuracy: 0.4861 - val_loss: 1.3893 - val_accuracy: 0.4646

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 216s 127ms/step - loss: 1.2820 - accuracy: 0.5056 - val_loss: 1.3776 - val_accuracy: 0.4104

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 214s 126ms/step - loss: 1.2922 - accuracy: 0.5097 - val_loss: 1.5660 - val_accuracy: 0.4175

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 179s 106ms/step - loss: 1.2616 - accuracy: 0.5068 - val_loss: 1.3656 - val_accuracy: 0.4387

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 194s 115ms/step - loss: 1.2831 - accuracy: 0.4867 - val_loss: 1.4359 - val_accuracy: 0.4104

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 177s 104ms/step - loss: 1.2728 - accuracy: 0.4938 - val_loss: 1.3283 - val_accuracy: 0.4623

Epoch 00031: val_loss improved from 1.34517 to 1.32826, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 32/50
1695/1695 [==============================] - 204s 121ms/step - loss: 1.2559 - accuracy: 0.5009 - val_loss: 1.3506 - val_accuracy: 0.4599

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 191s 113ms/step - loss: 1.2349 - accuracy: 0.5091 - val_loss: 1.4076 - val_accuracy: 0.4410

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 187s 110ms/step - loss: 1.2410 - accuracy: 0.5103 - val_loss: 1.3836 - val_accuracy: 0.4434

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 221s 131ms/step - loss: 1.2113 - accuracy: 0.5345 - val_loss: 1.3405 - val_accuracy: 0.4670

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 216s 128ms/step - loss: 1.1959 - accuracy: 0.5369 - val_loss: 1.3821 - val_accuracy: 0.4623

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 192s 113ms/step - loss: 1.2125 - accuracy: 0.5386 - val_loss: 1.3662 - val_accuracy: 0.4741

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 206s 121ms/step - loss: 1.1715 - accuracy: 0.5481 - val_loss: 1.3559 - val_accuracy: 0.4599

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 230s 135ms/step - loss: 1.1978 - accuracy: 0.5245 - val_loss: 1.3394 - val_accuracy: 0.4623

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 203s 120ms/step - loss: 1.1769 - accuracy: 0.5398 - val_loss: 1.4000 - val_accuracy: 0.4057

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 188s 111ms/step - loss: 1.1687 - accuracy: 0.5581 - val_loss: 1.3526 - val_accuracy: 0.4693

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 188s 111ms/step - loss: 1.1638 - accuracy: 0.5327 - val_loss: 1.3652 - val_accuracy: 0.4481

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 188s 111ms/step - loss: 1.1515 - accuracy: 0.5510 - val_loss: 1.3634 - val_accuracy: 0.4599

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 192s 113ms/step - loss: 1.1601 - accuracy: 0.5575 - val_loss: 1.3233 - val_accuracy: 0.4858

Epoch 00044: val_loss improved from 1.32826 to 1.32333, saving model to convdropout0.5densedropout0.4weights.hdf5
Epoch 45/50
1695/1695 [==============================] - 187s 110ms/step - loss: 1.1323 - accuracy: 0.5634 - val_loss: 1.3948 - val_accuracy: 0.4434

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 162s 95ms/step - loss: 1.1147 - accuracy: 0.5735 - val_loss: 1.3617 - val_accuracy: 0.4646

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 161s 95ms/step - loss: 1.1605 - accuracy: 0.5528 - val_loss: 1.3757 - val_accuracy: 0.4363

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.1179 - accuracy: 0.5723 - val_loss: 1.4222 - val_accuracy: 0.4623

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.1048 - accuracy: 0.5693 - val_loss: 1.3441 - val_accuracy: 0.4552

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.0937 - accuracy: 0.5723 - val_loss: 1.3467 - val_accuracy: 0.4858

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1069_IOM_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1033_WSI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1004_ITH_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1084_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1036_TIE_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1078_TIE_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3450611381602466
Dev set accuracy: 0.44736841320991516
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1057_IEO_ANG_MD.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1072_TIE_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1050_MTI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1041_TIE_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1022_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1080_IEO_SAD_HI.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_11 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_12 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_13 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_14 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_15 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 161s 95ms/step - loss: 2.6804 - accuracy: 0.2596 - val_loss: 2.3442 - val_accuracy: 0.1816

Epoch 00001: val_loss improved from inf to 2.34424, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0954 - accuracy: 0.2850 - val_loss: 2.3037 - val_accuracy: 0.2382

Epoch 00002: val_loss improved from 2.34424 to 2.30374, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9921 - accuracy: 0.3115 - val_loss: 1.9592 - val_accuracy: 0.3349

Epoch 00003: val_loss improved from 2.30374 to 1.95922, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.8497 - accuracy: 0.3286 - val_loss: 1.7468 - val_accuracy: 0.2995

Epoch 00004: val_loss improved from 1.95922 to 1.74679, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.8233 - accuracy: 0.3198 - val_loss: 1.7013 - val_accuracy: 0.2807

Epoch 00005: val_loss improved from 1.74679 to 1.70133, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.7643 - accuracy: 0.3345 - val_loss: 1.6177 - val_accuracy: 0.3608

Epoch 00006: val_loss improved from 1.70133 to 1.61766, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.6693 - accuracy: 0.3587 - val_loss: 1.6007 - val_accuracy: 0.3703

Epoch 00007: val_loss improved from 1.61766 to 1.60074, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 8/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.6330 - accuracy: 0.3581 - val_loss: 1.4321 - val_accuracy: 0.4245

Epoch 00008: val_loss improved from 1.60074 to 1.43206, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.5394 - accuracy: 0.3923 - val_loss: 1.4685 - val_accuracy: 0.3774

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 162s 96ms/step - loss: 1.5600 - accuracy: 0.3805 - val_loss: 1.4673 - val_accuracy: 0.3939

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 153s 91ms/step - loss: 1.5257 - accuracy: 0.4012 - val_loss: 1.5263 - val_accuracy: 0.3703

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.5234 - accuracy: 0.3953 - val_loss: 1.5033 - val_accuracy: 0.4033

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.5119 - accuracy: 0.4047 - val_loss: 1.4011 - val_accuracy: 0.4269

Epoch 00013: val_loss improved from 1.43206 to 1.40108, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 14/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.4665 - accuracy: 0.4242 - val_loss: 1.3993 - val_accuracy: 0.4458

Epoch 00014: val_loss improved from 1.40108 to 1.39931, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 15/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4692 - accuracy: 0.4147 - val_loss: 1.3948 - val_accuracy: 0.4340

Epoch 00015: val_loss improved from 1.39931 to 1.39477, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.4302 - accuracy: 0.4307 - val_loss: 1.3437 - val_accuracy: 0.4599

Epoch 00016: val_loss improved from 1.39477 to 1.34366, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 17/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.4222 - accuracy: 0.4301 - val_loss: 1.3867 - val_accuracy: 0.4080

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 161s 95ms/step - loss: 1.3839 - accuracy: 0.4448 - val_loss: 1.4213 - val_accuracy: 0.4269

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3630 - accuracy: 0.4673 - val_loss: 1.3339 - val_accuracy: 0.4410

Epoch 00019: val_loss improved from 1.34366 to 1.33389, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 20/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3694 - accuracy: 0.4466 - val_loss: 1.3552 - val_accuracy: 0.4599

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3672 - accuracy: 0.4631 - val_loss: 1.3387 - val_accuracy: 0.4693

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3490 - accuracy: 0.4891 - val_loss: 1.3389 - val_accuracy: 0.4363

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3354 - accuracy: 0.4637 - val_loss: 1.4206 - val_accuracy: 0.4292

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2997 - accuracy: 0.4814 - val_loss: 1.3638 - val_accuracy: 0.4198

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2909 - accuracy: 0.4850 - val_loss: 1.3068 - val_accuracy: 0.4835

Epoch 00025: val_loss improved from 1.33389 to 1.30676, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 26/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3082 - accuracy: 0.4838 - val_loss: 1.3103 - val_accuracy: 0.4764

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2638 - accuracy: 0.5044 - val_loss: 1.3794 - val_accuracy: 0.4623

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2790 - accuracy: 0.4950 - val_loss: 1.3059 - val_accuracy: 0.4670

Epoch 00028: val_loss improved from 1.30676 to 1.30595, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 29/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2828 - accuracy: 0.5015 - val_loss: 1.3670 - val_accuracy: 0.4222

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2602 - accuracy: 0.5086 - val_loss: 1.3742 - val_accuracy: 0.4316

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2600 - accuracy: 0.5109 - val_loss: 1.2581 - val_accuracy: 0.4811

Epoch 00031: val_loss improved from 1.30595 to 1.25807, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 32/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2544 - accuracy: 0.5239 - val_loss: 1.3548 - val_accuracy: 0.4104

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2090 - accuracy: 0.5304 - val_loss: 1.3179 - val_accuracy: 0.4929

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2064 - accuracy: 0.5292 - val_loss: 1.3499 - val_accuracy: 0.5142

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2175 - accuracy: 0.5304 - val_loss: 1.2491 - val_accuracy: 0.4953

Epoch 00035: val_loss improved from 1.25807 to 1.24910, saving model to convdropout0.5densedropout0.2weights.hdf5
Epoch 36/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1785 - accuracy: 0.5327 - val_loss: 1.2566 - val_accuracy: 0.4882

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2235 - accuracy: 0.5286 - val_loss: 1.3472 - val_accuracy: 0.4693

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1749 - accuracy: 0.5558 - val_loss: 1.3032 - val_accuracy: 0.4693

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1772 - accuracy: 0.5540 - val_loss: 1.3288 - val_accuracy: 0.4410

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1503 - accuracy: 0.5534 - val_loss: 1.3274 - val_accuracy: 0.4858

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1460 - accuracy: 0.5652 - val_loss: 1.4247 - val_accuracy: 0.4175

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1511 - accuracy: 0.5593 - val_loss: 1.3449 - val_accuracy: 0.4623

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1244 - accuracy: 0.5593 - val_loss: 1.4451 - val_accuracy: 0.4316

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1652 - accuracy: 0.5611 - val_loss: 1.3431 - val_accuracy: 0.4410

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1190 - accuracy: 0.5770 - val_loss: 1.3453 - val_accuracy: 0.4717

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1218 - accuracy: 0.5575 - val_loss: 1.3842 - val_accuracy: 0.4505

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1001 - accuracy: 0.5729 - val_loss: 1.3135 - val_accuracy: 0.4693

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1101 - accuracy: 0.5687 - val_loss: 1.3118 - val_accuracy: 0.4434

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1173 - accuracy: 0.5664 - val_loss: 1.3976 - val_accuracy: 0.4528

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.0784 - accuracy: 0.6006 - val_loss: 1.3003 - val_accuracy: 0.4858

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1047_TSI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1074_ITH_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1082_DFA_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1034_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1019_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1071_IWL_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.35775605717996
Dev set accuracy: 0.4661654233932495
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1026_IWW_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1080_TAI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1054_IOM_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1071_TAI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1019_WSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1020_IWW_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_16 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_17 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_13 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_15 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_18 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_14 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_19 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_15 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_20 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_16 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 162s 96ms/step - loss: 2.8798 - accuracy: 0.2643 - val_loss: 2.2840 - val_accuracy: 0.2429

Epoch 00001: val_loss improved from inf to 2.28404, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 157s 92ms/step - loss: 2.0286 - accuracy: 0.2950 - val_loss: 1.7819 - val_accuracy: 0.3302

Epoch 00002: val_loss improved from 2.28404 to 1.78185, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.9510 - accuracy: 0.3003 - val_loss: 1.5204 - val_accuracy: 0.3467

Epoch 00003: val_loss improved from 1.78185 to 1.52045, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.8435 - accuracy: 0.3339 - val_loss: 2.1572 - val_accuracy: 0.2311

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.8013 - accuracy: 0.3174 - val_loss: 1.4666 - val_accuracy: 0.3868

Epoch 00005: val_loss improved from 1.52045 to 1.46662, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7086 - accuracy: 0.3339 - val_loss: 1.5675 - val_accuracy: 0.2995

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6674 - accuracy: 0.3510 - val_loss: 1.4567 - val_accuracy: 0.4057

Epoch 00007: val_loss improved from 1.46662 to 1.45672, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 8/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.6333 - accuracy: 0.3540 - val_loss: 1.4012 - val_accuracy: 0.4741

Epoch 00008: val_loss improved from 1.45672 to 1.40116, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5951 - accuracy: 0.3847 - val_loss: 1.4915 - val_accuracy: 0.4434

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5831 - accuracy: 0.3587 - val_loss: 1.4635 - val_accuracy: 0.4387

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5337 - accuracy: 0.3788 - val_loss: 1.4107 - val_accuracy: 0.4481

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.4881 - accuracy: 0.4024 - val_loss: 1.4435 - val_accuracy: 0.4575

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5080 - accuracy: 0.4000 - val_loss: 1.4213 - val_accuracy: 0.4340

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.4525 - accuracy: 0.4165 - val_loss: 1.4159 - val_accuracy: 0.4481

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.4405 - accuracy: 0.4112 - val_loss: 1.3451 - val_accuracy: 0.4929

Epoch 00015: val_loss improved from 1.40116 to 1.34512, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.4197 - accuracy: 0.4265 - val_loss: 1.3451 - val_accuracy: 0.4599

Epoch 00016: val_loss improved from 1.34512 to 1.34511, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 17/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.4107 - accuracy: 0.4295 - val_loss: 1.3263 - val_accuracy: 0.5189

Epoch 00017: val_loss improved from 1.34511 to 1.32634, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 18/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.4204 - accuracy: 0.4354 - val_loss: 1.3293 - val_accuracy: 0.5094

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3879 - accuracy: 0.4525 - val_loss: 1.2861 - val_accuracy: 0.5307

Epoch 00019: val_loss improved from 1.32634 to 1.28613, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 20/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3781 - accuracy: 0.4478 - val_loss: 1.3130 - val_accuracy: 0.5142

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3849 - accuracy: 0.4472 - val_loss: 1.3558 - val_accuracy: 0.4882

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3479 - accuracy: 0.4507 - val_loss: 1.3042 - val_accuracy: 0.5118

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3375 - accuracy: 0.4779 - val_loss: 1.3201 - val_accuracy: 0.4858

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 170s 100ms/step - loss: 1.3428 - accuracy: 0.4649 - val_loss: 1.2744 - val_accuracy: 0.5330

Epoch 00024: val_loss improved from 1.28613 to 1.27443, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 25/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3155 - accuracy: 0.4749 - val_loss: 1.2832 - val_accuracy: 0.5236

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3149 - accuracy: 0.4696 - val_loss: 1.2454 - val_accuracy: 0.5448

Epoch 00026: val_loss improved from 1.27443 to 1.24537, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 27/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2877 - accuracy: 0.4968 - val_loss: 1.2796 - val_accuracy: 0.5189

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.3191 - accuracy: 0.4838 - val_loss: 1.2378 - val_accuracy: 0.5307

Epoch 00028: val_loss improved from 1.24537 to 1.23781, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 29/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2648 - accuracy: 0.5097 - val_loss: 1.2716 - val_accuracy: 0.5236

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2777 - accuracy: 0.5115 - val_loss: 1.2541 - val_accuracy: 0.5330

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2694 - accuracy: 0.5009 - val_loss: 1.3004 - val_accuracy: 0.4811

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2656 - accuracy: 0.5050 - val_loss: 1.2638 - val_accuracy: 0.5165

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2338 - accuracy: 0.5186 - val_loss: 1.2353 - val_accuracy: 0.5354

Epoch 00033: val_loss improved from 1.23781 to 1.23525, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 34/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2344 - accuracy: 0.5251 - val_loss: 1.3752 - val_accuracy: 0.4906

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2590 - accuracy: 0.4938 - val_loss: 1.2598 - val_accuracy: 0.5212

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2283 - accuracy: 0.5221 - val_loss: 1.2438 - val_accuracy: 0.5307

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2192 - accuracy: 0.5150 - val_loss: 1.2768 - val_accuracy: 0.5307

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2011 - accuracy: 0.5198 - val_loss: 1.3064 - val_accuracy: 0.5189

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2137 - accuracy: 0.5239 - val_loss: 1.2572 - val_accuracy: 0.5307

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1698 - accuracy: 0.5569 - val_loss: 1.2564 - val_accuracy: 0.5401

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1922 - accuracy: 0.5310 - val_loss: 1.2209 - val_accuracy: 0.5684

Epoch 00041: val_loss improved from 1.23525 to 1.22094, saving model to convdropout0.3densedropout0.6weights.hdf5
Epoch 42/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1678 - accuracy: 0.5451 - val_loss: 1.3247 - val_accuracy: 0.4835

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1891 - accuracy: 0.5251 - val_loss: 1.2571 - val_accuracy: 0.5354

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1775 - accuracy: 0.5363 - val_loss: 1.4141 - val_accuracy: 0.5330

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.1642 - accuracy: 0.5434 - val_loss: 1.2293 - val_accuracy: 0.5472

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1621 - accuracy: 0.5640 - val_loss: 1.2570 - val_accuracy: 0.5236

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1626 - accuracy: 0.5558 - val_loss: 1.2462 - val_accuracy: 0.5354

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.1097 - accuracy: 0.5717 - val_loss: 1.2248 - val_accuracy: 0.5566

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1397 - accuracy: 0.5634 - val_loss: 1.2303 - val_accuracy: 0.5283

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1235 - accuracy: 0.5563 - val_loss: 1.2947 - val_accuracy: 0.5401

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1051_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1074_IEO_DIS_HI.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1074_IWL_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1087_IEO_HAP_HI.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1019_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1002_DFA_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3922847527310365
Dev set accuracy: 0.48120301961898804
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1008_TSI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1040_IWW_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1018_ITS_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1067_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1039_ITS_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1036_DFA_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_21 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_18 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_22 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_17 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_19 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_23 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_18 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_15 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_20 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_24 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_19 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_5 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_9 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_25 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_20 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_10 (Dense)             (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 162s 95ms/step - loss: 2.8776 - accuracy: 0.2366 - val_loss: 1.9682 - val_accuracy: 0.2807

Epoch 00001: val_loss improved from inf to 1.96820, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 156s 92ms/step - loss: 2.1055 - accuracy: 0.2779 - val_loss: 1.7817 - val_accuracy: 0.1887

Epoch 00002: val_loss improved from 1.96820 to 1.78172, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.9761 - accuracy: 0.2973 - val_loss: 1.5815 - val_accuracy: 0.3585

Epoch 00003: val_loss improved from 1.78172 to 1.58154, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.8778 - accuracy: 0.3156 - val_loss: 1.9148 - val_accuracy: 0.2288

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7740 - accuracy: 0.3316 - val_loss: 1.5962 - val_accuracy: 0.3042

Epoch 00005: val_loss did not improve
Epoch 6/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7473 - accuracy: 0.3363 - val_loss: 1.4371 - val_accuracy: 0.4387

Epoch 00006: val_loss improved from 1.58154 to 1.43707, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6792 - accuracy: 0.3428 - val_loss: 1.5954 - val_accuracy: 0.3538

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6542 - accuracy: 0.3558 - val_loss: 1.4225 - val_accuracy: 0.4481

Epoch 00008: val_loss improved from 1.43707 to 1.42253, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.5633 - accuracy: 0.3841 - val_loss: 1.4169 - val_accuracy: 0.4741

Epoch 00009: val_loss improved from 1.42253 to 1.41685, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.5845 - accuracy: 0.3699 - val_loss: 1.3467 - val_accuracy: 0.4858

Epoch 00010: val_loss improved from 1.41685 to 1.34670, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.5175 - accuracy: 0.3853 - val_loss: 1.4973 - val_accuracy: 0.3703

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.5113 - accuracy: 0.3935 - val_loss: 1.5239 - val_accuracy: 0.4410

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4831 - accuracy: 0.4059 - val_loss: 1.3679 - val_accuracy: 0.4575

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4596 - accuracy: 0.4059 - val_loss: 1.3542 - val_accuracy: 0.5024

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4552 - accuracy: 0.4041 - val_loss: 1.3325 - val_accuracy: 0.4835

Epoch 00015: val_loss improved from 1.34670 to 1.33251, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4295 - accuracy: 0.4201 - val_loss: 1.3310 - val_accuracy: 0.5047

Epoch 00016: val_loss improved from 1.33251 to 1.33099, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 17/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4229 - accuracy: 0.4289 - val_loss: 1.3653 - val_accuracy: 0.4387

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4034 - accuracy: 0.4460 - val_loss: 1.3821 - val_accuracy: 0.4363

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3697 - accuracy: 0.4673 - val_loss: 1.3000 - val_accuracy: 0.4953

Epoch 00019: val_loss improved from 1.33099 to 1.30001, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 20/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.3729 - accuracy: 0.4478 - val_loss: 1.3298 - val_accuracy: 0.4858

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.3601 - accuracy: 0.4566 - val_loss: 1.3143 - val_accuracy: 0.4764

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3513 - accuracy: 0.4637 - val_loss: 1.3511 - val_accuracy: 0.4505

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3582 - accuracy: 0.4690 - val_loss: 1.3133 - val_accuracy: 0.4882

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3160 - accuracy: 0.4743 - val_loss: 1.3052 - val_accuracy: 0.4929

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3340 - accuracy: 0.4661 - val_loss: 1.2777 - val_accuracy: 0.4906

Epoch 00025: val_loss improved from 1.30001 to 1.27769, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 26/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2976 - accuracy: 0.4873 - val_loss: 1.3037 - val_accuracy: 0.4858

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2954 - accuracy: 0.4802 - val_loss: 1.2957 - val_accuracy: 0.4764

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2864 - accuracy: 0.5015 - val_loss: 1.2980 - val_accuracy: 0.4858

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2821 - accuracy: 0.4879 - val_loss: 1.2864 - val_accuracy: 0.5094

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2798 - accuracy: 0.4861 - val_loss: 1.2692 - val_accuracy: 0.4953

Epoch 00030: val_loss improved from 1.27769 to 1.26924, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 31/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2956 - accuracy: 0.4985 - val_loss: 1.2892 - val_accuracy: 0.4646

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2937 - accuracy: 0.4844 - val_loss: 1.3860 - val_accuracy: 0.4340

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2614 - accuracy: 0.4932 - val_loss: 1.3460 - val_accuracy: 0.4575

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2439 - accuracy: 0.5209 - val_loss: 1.2537 - val_accuracy: 0.5024

Epoch 00034: val_loss improved from 1.26924 to 1.25370, saving model to convdropout0.3densedropout0.4weights.hdf5
Epoch 35/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.2392 - accuracy: 0.5150 - val_loss: 1.2653 - val_accuracy: 0.5094

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2091 - accuracy: 0.5268 - val_loss: 1.2866 - val_accuracy: 0.5047

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2227 - accuracy: 0.5227 - val_loss: 1.3650 - val_accuracy: 0.4528

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2094 - accuracy: 0.5186 - val_loss: 1.3038 - val_accuracy: 0.4882

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2263 - accuracy: 0.5263 - val_loss: 1.3130 - val_accuracy: 0.5071

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1899 - accuracy: 0.5381 - val_loss: 1.2648 - val_accuracy: 0.5236

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1859 - accuracy: 0.5469 - val_loss: 1.3331 - val_accuracy: 0.4528

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2053 - accuracy: 0.5251 - val_loss: 1.2910 - val_accuracy: 0.4858

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1590 - accuracy: 0.5304 - val_loss: 1.4331 - val_accuracy: 0.3774

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 162s 95ms/step - loss: 1.1641 - accuracy: 0.5469 - val_loss: 1.2564 - val_accuracy: 0.5071

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 165s 97ms/step - loss: 1.1647 - accuracy: 0.5434 - val_loss: 1.2706 - val_accuracy: 0.5000

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.1493 - accuracy: 0.5540 - val_loss: 1.3373 - val_accuracy: 0.4717

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.1089 - accuracy: 0.5758 - val_loss: 1.2826 - val_accuracy: 0.4882

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.1382 - accuracy: 0.5705 - val_loss: 1.3398 - val_accuracy: 0.4764

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.1294 - accuracy: 0.5764 - val_loss: 1.2702 - val_accuracy: 0.5354

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.1210 - accuracy: 0.5581 - val_loss: 1.2806 - val_accuracy: 0.5189

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1040_TAI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1033_WSI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1004_ITH_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1022_IWW_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1088_IOM_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1049_ITS_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3541651408475144
Dev set accuracy: 0.5075187683105469
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1073_IEO_ANG_MD.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1014_TAI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1023_IWW_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1081_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1069_ITS_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1084_IEO_SAD_HI.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_21 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_26 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_22 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_27 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_21 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_23 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_28 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_22 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_24 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_29 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_23 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_11 (Dense)             (None, 128)               639104    
_________________________________________________________________
activation_30 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_24 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_12 (Dense)             (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 165s 98ms/step - loss: 2.8164 - accuracy: 0.2425 - val_loss: 1.8473 - val_accuracy: 0.2123

Epoch 00001: val_loss improved from inf to 1.84734, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 160s 94ms/step - loss: 2.1450 - accuracy: 0.2761 - val_loss: 1.7110 - val_accuracy: 0.3844

Epoch 00002: val_loss improved from 1.84734 to 1.71097, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.9828 - accuracy: 0.2973 - val_loss: 1.8222 - val_accuracy: 0.3066

Epoch 00003: val_loss did not improve
Epoch 4/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.9030 - accuracy: 0.3133 - val_loss: 1.6789 - val_accuracy: 0.2665

Epoch 00004: val_loss improved from 1.71097 to 1.67888, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.7576 - accuracy: 0.3280 - val_loss: 1.5683 - val_accuracy: 0.4410

Epoch 00005: val_loss improved from 1.67888 to 1.56825, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.7273 - accuracy: 0.3327 - val_loss: 1.5832 - val_accuracy: 0.3019

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 162s 96ms/step - loss: 1.6605 - accuracy: 0.3717 - val_loss: 1.5963 - val_accuracy: 0.4245

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.5987 - accuracy: 0.3770 - val_loss: 1.4864 - val_accuracy: 0.4127

Epoch 00008: val_loss improved from 1.56825 to 1.48637, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.6195 - accuracy: 0.3628 - val_loss: 1.4237 - val_accuracy: 0.4410

Epoch 00009: val_loss improved from 1.48637 to 1.42374, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.5698 - accuracy: 0.3752 - val_loss: 1.4558 - val_accuracy: 0.4151

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.5651 - accuracy: 0.3687 - val_loss: 1.3940 - val_accuracy: 0.4764

Epoch 00011: val_loss improved from 1.42374 to 1.39396, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 12/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.5124 - accuracy: 0.3959 - val_loss: 1.4799 - val_accuracy: 0.4434

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.5173 - accuracy: 0.3965 - val_loss: 1.3688 - val_accuracy: 0.4858

Epoch 00013: val_loss improved from 1.39396 to 1.36879, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 14/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.4785 - accuracy: 0.3917 - val_loss: 1.4126 - val_accuracy: 0.3962

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.4497 - accuracy: 0.4077 - val_loss: 1.3322 - val_accuracy: 0.5071

Epoch 00015: val_loss improved from 1.36879 to 1.33215, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.4358 - accuracy: 0.4142 - val_loss: 1.3756 - val_accuracy: 0.4693

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 164s 97ms/step - loss: 1.4338 - accuracy: 0.4383 - val_loss: 1.4049 - val_accuracy: 0.4269

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 163s 96ms/step - loss: 1.4079 - accuracy: 0.4348 - val_loss: 1.3649 - val_accuracy: 0.4599

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.3879 - accuracy: 0.4407 - val_loss: 1.4545 - val_accuracy: 0.4575

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.3895 - accuracy: 0.4419 - val_loss: 1.2691 - val_accuracy: 0.4929

Epoch 00020: val_loss improved from 1.33215 to 1.26908, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 21/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3796 - accuracy: 0.4513 - val_loss: 1.3196 - val_accuracy: 0.4953

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3613 - accuracy: 0.4484 - val_loss: 1.3424 - val_accuracy: 0.4528

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3586 - accuracy: 0.4401 - val_loss: 1.3335 - val_accuracy: 0.4646

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3617 - accuracy: 0.4490 - val_loss: 1.3667 - val_accuracy: 0.4717

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3203 - accuracy: 0.4637 - val_loss: 1.3096 - val_accuracy: 0.5047

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3229 - accuracy: 0.4614 - val_loss: 1.2981 - val_accuracy: 0.4929

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3087 - accuracy: 0.4779 - val_loss: 1.2806 - val_accuracy: 0.4882

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3141 - accuracy: 0.4779 - val_loss: 1.3099 - val_accuracy: 0.4764

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2956 - accuracy: 0.4979 - val_loss: 1.3150 - val_accuracy: 0.4670

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.2945 - accuracy: 0.4808 - val_loss: 1.3465 - val_accuracy: 0.4646

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2792 - accuracy: 0.4826 - val_loss: 1.3181 - val_accuracy: 0.4976

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2651 - accuracy: 0.4950 - val_loss: 1.2972 - val_accuracy: 0.4906

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2766 - accuracy: 0.4985 - val_loss: 1.3008 - val_accuracy: 0.5118

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2641 - accuracy: 0.5009 - val_loss: 1.2944 - val_accuracy: 0.4953

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2545 - accuracy: 0.5139 - val_loss: 1.2654 - val_accuracy: 0.5236

Epoch 00035: val_loss improved from 1.26908 to 1.26542, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 36/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.2439 - accuracy: 0.4932 - val_loss: 1.2613 - val_accuracy: 0.4953

Epoch 00036: val_loss improved from 1.26542 to 1.26127, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 37/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2346 - accuracy: 0.5080 - val_loss: 1.5479 - val_accuracy: 0.3491

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2209 - accuracy: 0.5180 - val_loss: 1.2772 - val_accuracy: 0.5024

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2226 - accuracy: 0.5363 - val_loss: 1.2592 - val_accuracy: 0.4976

Epoch 00039: val_loss improved from 1.26127 to 1.25919, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 40/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2160 - accuracy: 0.5198 - val_loss: 1.5493 - val_accuracy: 0.4481

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.1884 - accuracy: 0.5499 - val_loss: 1.2733 - val_accuracy: 0.5236

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.1995 - accuracy: 0.5274 - val_loss: 1.4374 - val_accuracy: 0.4434

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2019 - accuracy: 0.5286 - val_loss: 1.2470 - val_accuracy: 0.5094

Epoch 00043: val_loss improved from 1.25919 to 1.24697, saving model to convdropout0.3densedropout0.2weights.hdf5
Epoch 44/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1527 - accuracy: 0.5487 - val_loss: 1.2862 - val_accuracy: 0.5259

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.1809 - accuracy: 0.5310 - val_loss: 1.2495 - val_accuracy: 0.4976

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1612 - accuracy: 0.5504 - val_loss: 1.3198 - val_accuracy: 0.4929

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1529 - accuracy: 0.5493 - val_loss: 1.2936 - val_accuracy: 0.4906

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.1481 - accuracy: 0.5546 - val_loss: 1.2632 - val_accuracy: 0.5118

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1339 - accuracy: 0.5628 - val_loss: 1.2570 - val_accuracy: 0.5165

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1351 - accuracy: 0.5622 - val_loss: 1.2579 - val_accuracy: 0.5142

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1013_WSI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1088_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1007_IEO_FEA_MD.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1041_IEO_HAP_LO.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1078_IEO_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1022_IEO_SAD_MD.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.4209089942444535
Dev set accuracy: 0.4774436056613922
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1028_IWL_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1076_TAI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1044_IOM_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1013_IEO_HAP_LO.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1034_ITH_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1056_IWL_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_25 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_31 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_26 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_32 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_25 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_20 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_27 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_33 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_26 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_28 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_34 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_27 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_7 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 128)               639104    
_________________________________________________________________
activation_35 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_28 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_14 (Dense)             (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 157s 92ms/step - loss: 2.8663 - accuracy: 0.2661 - val_loss: 2.1743 - val_accuracy: 0.1557

Epoch 00001: val_loss improved from inf to 2.17433, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.0078 - accuracy: 0.2962 - val_loss: 2.4436 - val_accuracy: 0.1981

Epoch 00002: val_loss did not improve
Epoch 3/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0094 - accuracy: 0.2879 - val_loss: 1.6612 - val_accuracy: 0.3420

Epoch 00003: val_loss improved from 2.17433 to 1.66118, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.8569 - accuracy: 0.3233 - val_loss: 1.9777 - val_accuracy: 0.2759

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 151s 89ms/step - loss: 1.7449 - accuracy: 0.3510 - val_loss: 1.5465 - val_accuracy: 0.3443

Epoch 00005: val_loss improved from 1.66118 to 1.54649, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.7348 - accuracy: 0.3569 - val_loss: 1.5428 - val_accuracy: 0.4104

Epoch 00006: val_loss improved from 1.54649 to 1.54284, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.6315 - accuracy: 0.3516 - val_loss: 1.5790 - val_accuracy: 0.4080

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 151s 89ms/step - loss: 1.5895 - accuracy: 0.3912 - val_loss: 1.5095 - val_accuracy: 0.4151

Epoch 00008: val_loss improved from 1.54284 to 1.50952, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.5474 - accuracy: 0.3941 - val_loss: 1.5672 - val_accuracy: 0.3656

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.5364 - accuracy: 0.3805 - val_loss: 1.5064 - val_accuracy: 0.4269

Epoch 00010: val_loss improved from 1.50952 to 1.50641, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4735 - accuracy: 0.4088 - val_loss: 1.5146 - val_accuracy: 0.4080

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4930 - accuracy: 0.4071 - val_loss: 1.4668 - val_accuracy: 0.4528

Epoch 00012: val_loss improved from 1.50641 to 1.46683, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 13/50
1695/1695 [==============================] - 151s 89ms/step - loss: 1.4448 - accuracy: 0.4271 - val_loss: 1.4180 - val_accuracy: 0.4505

Epoch 00013: val_loss improved from 1.46683 to 1.41798, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 14/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4155 - accuracy: 0.4395 - val_loss: 1.5496 - val_accuracy: 0.3844

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 162s 95ms/step - loss: 1.3792 - accuracy: 0.4596 - val_loss: 1.4253 - val_accuracy: 0.4670

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.3955 - accuracy: 0.4625 - val_loss: 1.5420 - val_accuracy: 0.4033

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3521 - accuracy: 0.4702 - val_loss: 1.5434 - val_accuracy: 0.4245

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3508 - accuracy: 0.4631 - val_loss: 1.3859 - val_accuracy: 0.4410

Epoch 00018: val_loss improved from 1.41798 to 1.38592, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 19/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3185 - accuracy: 0.4820 - val_loss: 1.4345 - val_accuracy: 0.4505

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.3307 - accuracy: 0.4785 - val_loss: 1.4751 - val_accuracy: 0.4481

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3021 - accuracy: 0.4920 - val_loss: 1.3975 - val_accuracy: 0.4528

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2852 - accuracy: 0.5062 - val_loss: 1.4124 - val_accuracy: 0.4458

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2877 - accuracy: 0.4861 - val_loss: 1.4219 - val_accuracy: 0.4387

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.2852 - accuracy: 0.4985 - val_loss: 1.3813 - val_accuracy: 0.4693

Epoch 00024: val_loss improved from 1.38592 to 1.38135, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 25/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2446 - accuracy: 0.4985 - val_loss: 1.3746 - val_accuracy: 0.4387

Epoch 00025: val_loss improved from 1.38135 to 1.37464, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 26/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2397 - accuracy: 0.5245 - val_loss: 1.4632 - val_accuracy: 0.4009

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.2451 - accuracy: 0.5074 - val_loss: 1.3768 - val_accuracy: 0.4646

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.2374 - accuracy: 0.5198 - val_loss: 1.4086 - val_accuracy: 0.4434

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 153s 91ms/step - loss: 1.2099 - accuracy: 0.5369 - val_loss: 1.3959 - val_accuracy: 0.4623

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2084 - accuracy: 0.5221 - val_loss: 1.3802 - val_accuracy: 0.4434

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.1730 - accuracy: 0.5445 - val_loss: 1.5772 - val_accuracy: 0.3491

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.1876 - accuracy: 0.5245 - val_loss: 1.3542 - val_accuracy: 0.4528

Epoch 00032: val_loss improved from 1.37464 to 1.35417, saving model to convdropout0.1densedropout0.6weights.hdf5
Epoch 33/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1930 - accuracy: 0.5398 - val_loss: 1.4348 - val_accuracy: 0.4505

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1427 - accuracy: 0.5605 - val_loss: 1.3728 - val_accuracy: 0.4363

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1495 - accuracy: 0.5493 - val_loss: 1.4231 - val_accuracy: 0.4410

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1589 - accuracy: 0.5499 - val_loss: 1.4058 - val_accuracy: 0.4528

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1273 - accuracy: 0.5799 - val_loss: 1.4214 - val_accuracy: 0.4292

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1423 - accuracy: 0.5617 - val_loss: 1.3960 - val_accuracy: 0.4575

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 162s 95ms/step - loss: 1.1304 - accuracy: 0.5569 - val_loss: 1.4906 - val_accuracy: 0.4175

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.1234 - accuracy: 0.5687 - val_loss: 1.3993 - val_accuracy: 0.4458

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.0692 - accuracy: 0.5758 - val_loss: 1.3542 - val_accuracy: 0.4717

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.0599 - accuracy: 0.5870 - val_loss: 1.4565 - val_accuracy: 0.4599

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1094 - accuracy: 0.5723 - val_loss: 1.3911 - val_accuracy: 0.4670

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.0559 - accuracy: 0.5953 - val_loss: 1.3944 - val_accuracy: 0.4835

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.0829 - accuracy: 0.5876 - val_loss: 1.4912 - val_accuracy: 0.4599

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 153s 91ms/step - loss: 1.0752 - accuracy: 0.5912 - val_loss: 1.4481 - val_accuracy: 0.4151

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.0522 - accuracy: 0.5959 - val_loss: 1.3832 - val_accuracy: 0.4575

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.0164 - accuracy: 0.6088 - val_loss: 1.4030 - val_accuracy: 0.4387

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.0718 - accuracy: 0.5853 - val_loss: 1.5037 - val_accuracy: 0.4552

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.0431 - accuracy: 0.6029 - val_loss: 1.4646 - val_accuracy: 0.4340

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1090_IEO_ANG_LO.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1072_IOM_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1060_IEO_FEA_LO.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1076_IOM_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1037_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1037_IEO_SAD_LO.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3754234780046277
Dev set accuracy: 0.5338345766067505
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1033_IWL_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1085_ITH_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1044_TIE_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1021_TIE_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1067_TSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1060_IEO_SAD_LO.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_8"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_29 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_36 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_30 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_37 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_29 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_31 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_38 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_30 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_24 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_32 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_39 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_31 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_8 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_15 (Dense)             (None, 128)               639104    
_________________________________________________________________
activation_40 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_32 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_16 (Dense)             (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 160s 94ms/step - loss: 2.6899 - accuracy: 0.2566 - val_loss: 2.0264 - val_accuracy: 0.2594

Epoch 00001: val_loss improved from inf to 2.02640, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 157s 92ms/step - loss: 2.1089 - accuracy: 0.2844 - val_loss: 1.7342 - val_accuracy: 0.2453

Epoch 00002: val_loss improved from 2.02640 to 1.73421, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.9157 - accuracy: 0.3251 - val_loss: 2.1386 - val_accuracy: 0.3019

Epoch 00003: val_loss did not improve
Epoch 4/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.8778 - accuracy: 0.3056 - val_loss: 1.7039 - val_accuracy: 0.2854

Epoch 00004: val_loss improved from 1.73421 to 1.70385, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.7756 - accuracy: 0.3457 - val_loss: 1.4670 - val_accuracy: 0.4269

Epoch 00005: val_loss improved from 1.70385 to 1.46696, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.7283 - accuracy: 0.3611 - val_loss: 1.6375 - val_accuracy: 0.3137

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.6853 - accuracy: 0.3445 - val_loss: 1.5324 - val_accuracy: 0.3750

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.6296 - accuracy: 0.3634 - val_loss: 1.4359 - val_accuracy: 0.4104

Epoch 00008: val_loss improved from 1.46696 to 1.43589, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5900 - accuracy: 0.3858 - val_loss: 1.4113 - val_accuracy: 0.4434

Epoch 00009: val_loss improved from 1.43589 to 1.41129, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.5616 - accuracy: 0.3764 - val_loss: 1.4386 - val_accuracy: 0.4599

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.5412 - accuracy: 0.3941 - val_loss: 1.4776 - val_accuracy: 0.4222

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.4999 - accuracy: 0.4047 - val_loss: 1.4428 - val_accuracy: 0.4363

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 164s 97ms/step - loss: 1.4967 - accuracy: 0.4029 - val_loss: 1.6877 - val_accuracy: 0.3514

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.4524 - accuracy: 0.4289 - val_loss: 1.4985 - val_accuracy: 0.3821

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 164s 97ms/step - loss: 1.4444 - accuracy: 0.4218 - val_loss: 1.3583 - val_accuracy: 0.4811

Epoch 00015: val_loss improved from 1.41129 to 1.35829, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.3957 - accuracy: 0.4519 - val_loss: 1.3896 - val_accuracy: 0.4481

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.4089 - accuracy: 0.4448 - val_loss: 1.3966 - val_accuracy: 0.4575

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3589 - accuracy: 0.4543 - val_loss: 1.4814 - val_accuracy: 0.4269

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3788 - accuracy: 0.4566 - val_loss: 1.3473 - val_accuracy: 0.4858

Epoch 00019: val_loss improved from 1.35829 to 1.34729, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 20/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3738 - accuracy: 0.4625 - val_loss: 1.3393 - val_accuracy: 0.4788

Epoch 00020: val_loss improved from 1.34729 to 1.33932, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3574 - accuracy: 0.4590 - val_loss: 1.4226 - val_accuracy: 0.4340

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3510 - accuracy: 0.4678 - val_loss: 1.4103 - val_accuracy: 0.4387

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3171 - accuracy: 0.4808 - val_loss: 1.3888 - val_accuracy: 0.4599

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.3049 - accuracy: 0.4820 - val_loss: 1.4067 - val_accuracy: 0.4410

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.3134 - accuracy: 0.4867 - val_loss: 1.3665 - val_accuracy: 0.4670

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3062 - accuracy: 0.4867 - val_loss: 1.3528 - val_accuracy: 0.4646

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2946 - accuracy: 0.4867 - val_loss: 1.3188 - val_accuracy: 0.4858

Epoch 00027: val_loss improved from 1.33932 to 1.31884, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 28/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2643 - accuracy: 0.4985 - val_loss: 1.3914 - val_accuracy: 0.4175

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2727 - accuracy: 0.4944 - val_loss: 1.3690 - val_accuracy: 0.4575

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.2680 - accuracy: 0.4914 - val_loss: 1.3173 - val_accuracy: 0.4835

Epoch 00030: val_loss improved from 1.31884 to 1.31727, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 31/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2600 - accuracy: 0.5009 - val_loss: 1.3079 - val_accuracy: 0.4764

Epoch 00031: val_loss improved from 1.31727 to 1.30794, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 32/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2605 - accuracy: 0.5032 - val_loss: 1.3551 - val_accuracy: 0.4505

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.2205 - accuracy: 0.5245 - val_loss: 1.4132 - val_accuracy: 0.4269

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2299 - accuracy: 0.5263 - val_loss: 1.4033 - val_accuracy: 0.4670

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2242 - accuracy: 0.5056 - val_loss: 1.3870 - val_accuracy: 0.4316

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1988 - accuracy: 0.5263 - val_loss: 1.3376 - val_accuracy: 0.4741

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2173 - accuracy: 0.5268 - val_loss: 1.3517 - val_accuracy: 0.4340

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1955 - accuracy: 0.5316 - val_loss: 1.3102 - val_accuracy: 0.4976

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2010 - accuracy: 0.5369 - val_loss: 1.3186 - val_accuracy: 0.4764

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1923 - accuracy: 0.5375 - val_loss: 1.2997 - val_accuracy: 0.4953

Epoch 00040: val_loss improved from 1.30794 to 1.29971, saving model to convdropout0.1densedropout0.4weights.hdf5
Epoch 41/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1976 - accuracy: 0.5215 - val_loss: 1.3426 - val_accuracy: 0.4575

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1711 - accuracy: 0.5457 - val_loss: 1.3342 - val_accuracy: 0.4835

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1555 - accuracy: 0.5422 - val_loss: 1.3726 - val_accuracy: 0.4670

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1703 - accuracy: 0.5487 - val_loss: 1.3276 - val_accuracy: 0.4811

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1624 - accuracy: 0.5487 - val_loss: 1.3596 - val_accuracy: 0.4434

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1329 - accuracy: 0.5587 - val_loss: 1.3270 - val_accuracy: 0.4882

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1425 - accuracy: 0.5575 - val_loss: 1.4568 - val_accuracy: 0.4009

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1187 - accuracy: 0.5717 - val_loss: 1.4097 - val_accuracy: 0.4127

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.1324 - accuracy: 0.5805 - val_loss: 1.3105 - val_accuracy: 0.4929

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1129 - accuracy: 0.5569 - val_loss: 1.3570 - val_accuracy: 0.4646

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1029_IEO_ANG_HI.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1062_IEO_DIS_HI.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1003_TIE_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1087_TAI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1036_IEO_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1052_IWW_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3106453212580287
Dev set accuracy: 0.5112782120704651
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1055_TSI_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1014_TIE_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1032_WSI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1050_ITS_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1045_IOM_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1015_TSI_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_33 (MaxPooling (None, 48, 210, 32)       0         
_________________________________________________________________
activation_41 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_9 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_34 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_42 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_33 (Dropout)         (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_35 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_43 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_34 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_27 (Conv2D)           (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_36 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_44 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_35 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_9 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_17 (Dense)             (None, 128)               639104    
_________________________________________________________________
activation_45 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_36 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_18 (Dense)             (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 158s 93ms/step - loss: 2.8415 - accuracy: 0.2596 - val_loss: 1.9315 - val_accuracy: 0.2358

Epoch 00001: val_loss improved from inf to 1.93154, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 154s 91ms/step - loss: 2.1028 - accuracy: 0.2737 - val_loss: 1.8096 - val_accuracy: 0.2830

Epoch 00002: val_loss improved from 1.93154 to 1.80962, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0033 - accuracy: 0.2861 - val_loss: 1.6550 - val_accuracy: 0.3396

Epoch 00003: val_loss improved from 1.80962 to 1.65497, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.8804 - accuracy: 0.3180 - val_loss: 1.8066 - val_accuracy: 0.2995

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.8134 - accuracy: 0.3345 - val_loss: 1.7375 - val_accuracy: 0.3467

Epoch 00005: val_loss did not improve
Epoch 6/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.7499 - accuracy: 0.3375 - val_loss: 1.5748 - val_accuracy: 0.3868

Epoch 00006: val_loss improved from 1.65497 to 1.57484, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.7303 - accuracy: 0.3375 - val_loss: 1.5335 - val_accuracy: 0.3939

Epoch 00007: val_loss improved from 1.57484 to 1.53350, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 8/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6530 - accuracy: 0.3575 - val_loss: 1.4151 - val_accuracy: 0.4458

Epoch 00008: val_loss improved from 1.53350 to 1.41510, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 197s 116ms/step - loss: 1.5888 - accuracy: 0.3729 - val_loss: 1.4664 - val_accuracy: 0.4245

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.5926 - accuracy: 0.3811 - val_loss: 1.3826 - val_accuracy: 0.4693

Epoch 00010: val_loss improved from 1.41510 to 1.38257, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 163s 96ms/step - loss: 1.5273 - accuracy: 0.3858 - val_loss: 1.4677 - val_accuracy: 0.3585

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 176s 104ms/step - loss: 1.5166 - accuracy: 0.4041 - val_loss: 1.4709 - val_accuracy: 0.4080

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 173s 102ms/step - loss: 1.4666 - accuracy: 0.4224 - val_loss: 1.3876 - val_accuracy: 0.4528

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.4812 - accuracy: 0.3982 - val_loss: 1.4553 - val_accuracy: 0.4269

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.4757 - accuracy: 0.4142 - val_loss: 1.3449 - val_accuracy: 0.4693

Epoch 00015: val_loss improved from 1.38257 to 1.34488, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 16/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.4369 - accuracy: 0.4212 - val_loss: 1.4014 - val_accuracy: 0.3962

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 210s 124ms/step - loss: 1.4294 - accuracy: 0.4307 - val_loss: 1.3652 - val_accuracy: 0.4623

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 198s 117ms/step - loss: 1.4071 - accuracy: 0.4478 - val_loss: 1.4363 - val_accuracy: 0.4198

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 192s 113ms/step - loss: 1.3827 - accuracy: 0.4466 - val_loss: 1.4647 - val_accuracy: 0.3703

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 184s 108ms/step - loss: 1.3856 - accuracy: 0.4578 - val_loss: 1.3966 - val_accuracy: 0.4670

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 181s 107ms/step - loss: 1.3422 - accuracy: 0.4702 - val_loss: 1.2892 - val_accuracy: 0.4906

Epoch 00021: val_loss improved from 1.34488 to 1.28916, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 22/50
1695/1695 [==============================] - 181s 107ms/step - loss: 1.3869 - accuracy: 0.4448 - val_loss: 1.3529 - val_accuracy: 0.4717

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 184s 109ms/step - loss: 1.3272 - accuracy: 0.4779 - val_loss: 1.3183 - val_accuracy: 0.4835

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 189s 112ms/step - loss: 1.3382 - accuracy: 0.4832 - val_loss: 1.2775 - val_accuracy: 0.5047

Epoch 00024: val_loss improved from 1.28916 to 1.27754, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 25/50
1695/1695 [==============================] - 190s 112ms/step - loss: 1.2968 - accuracy: 0.4926 - val_loss: 1.3255 - val_accuracy: 0.4858

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 196s 115ms/step - loss: 1.2929 - accuracy: 0.5021 - val_loss: 1.3040 - val_accuracy: 0.5047

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 193s 114ms/step - loss: 1.2832 - accuracy: 0.4903 - val_loss: 1.3575 - val_accuracy: 0.4198

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 201s 119ms/step - loss: 1.2829 - accuracy: 0.4968 - val_loss: 1.3795 - val_accuracy: 0.4882

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.3006 - accuracy: 0.4903 - val_loss: 1.3116 - val_accuracy: 0.5000

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 180s 106ms/step - loss: 1.2741 - accuracy: 0.4932 - val_loss: 1.3080 - val_accuracy: 0.4953

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 192s 113ms/step - loss: 1.2586 - accuracy: 0.5062 - val_loss: 1.3100 - val_accuracy: 0.4882

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 193s 114ms/step - loss: 1.2366 - accuracy: 0.5245 - val_loss: 1.3542 - val_accuracy: 0.4434

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 254s 150ms/step - loss: 1.2470 - accuracy: 0.5227 - val_loss: 1.3424 - val_accuracy: 0.4741

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 226s 133ms/step - loss: 1.1953 - accuracy: 0.5481 - val_loss: 1.3559 - val_accuracy: 0.4575

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 198s 117ms/step - loss: 1.2314 - accuracy: 0.5139 - val_loss: 1.3412 - val_accuracy: 0.4481

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 203s 120ms/step - loss: 1.2138 - accuracy: 0.5139 - val_loss: 1.3340 - val_accuracy: 0.4906

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 164s 97ms/step - loss: 1.2151 - accuracy: 0.5245 - val_loss: 1.2951 - val_accuracy: 0.4741

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 177s 104ms/step - loss: 1.1871 - accuracy: 0.5422 - val_loss: 1.2617 - val_accuracy: 0.5047

Epoch 00038: val_loss improved from 1.27754 to 1.26166, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 39/50
1695/1695 [==============================] - 181s 107ms/step - loss: 1.1686 - accuracy: 0.5434 - val_loss: 1.3358 - val_accuracy: 0.4292

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 178s 105ms/step - loss: 1.2046 - accuracy: 0.5310 - val_loss: 1.2879 - val_accuracy: 0.4953

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 188s 111ms/step - loss: 1.1979 - accuracy: 0.5327 - val_loss: 1.2774 - val_accuracy: 0.5071

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 218s 129ms/step - loss: 1.1711 - accuracy: 0.5569 - val_loss: 1.3103 - val_accuracy: 0.4788

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 247s 146ms/step - loss: 1.1368 - accuracy: 0.5534 - val_loss: 1.3055 - val_accuracy: 0.5071

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 238s 140ms/step - loss: 1.1486 - accuracy: 0.5451 - val_loss: 1.2726 - val_accuracy: 0.4953

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 238s 140ms/step - loss: 1.1674 - accuracy: 0.5457 - val_loss: 1.3077 - val_accuracy: 0.5142

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 237s 140ms/step - loss: 1.1051 - accuracy: 0.5776 - val_loss: 1.2607 - val_accuracy: 0.5047

Epoch 00046: val_loss improved from 1.26166 to 1.26067, saving model to convdropout0.1densedropout0.2weights.hdf5
Epoch 47/50
1695/1695 [==============================] - 238s 141ms/step - loss: 1.1160 - accuracy: 0.5735 - val_loss: 1.3474 - val_accuracy: 0.4646

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 238s 140ms/step - loss: 1.1007 - accuracy: 0.5646 - val_loss: 1.3501 - val_accuracy: 0.4906

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 238s 141ms/step - loss: 1.1136 - accuracy: 0.5906 - val_loss: 1.3007 - val_accuracy: 0.4717

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 237s 140ms/step - loss: 1.0992 - accuracy: 0.5894 - val_loss: 1.3480 - val_accuracy: 0.4481

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1062_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1088_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1017_IEO_FEA_LO.wav.npz                  
 Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1074_ITS_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1055_IWL_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1033_ITS_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3556235172694786
Dev set accuracy: 0.4548872113227844
(tensorflow) DNa1c06a0:panotti evangelie$ 
