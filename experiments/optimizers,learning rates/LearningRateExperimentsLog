Last login: Sat Nov 16 10:21:11 on ttys000
(base) DN0a241607:~ evangelie$ cd ~/Documents/GitHub/panotti/
(base) DN0a241607:panotti evangelie$ source activate tensorflow
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1013_TAI_ANG_XX.wa Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1086_ITS_ANG_XX. Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1088_IEO_ANG_HI. Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1085_IWL_ANG_XX. Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1043_IWL_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1017_TSI_DIS_XX.wa Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1022_IEO_DIS_LO. Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1052_IEO_DIS_HI. Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1022_ITH_DIS_XX. Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1007_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1058_IEO_FEA_LO.wa Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1006_IWW_FEA_XX. Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1055_TAI_FEA_XX. Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1016_IEO_FEA_HI. Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1040_IWL_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1020_TIE_HAP_XX.wa Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1028_ITH_HAP_XX. Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1062_DFA_HAP_XX. Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1078_IWL_HAP_XX. Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1023_IWL_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1081_TIE_NEU_XX.wa Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1032_IOM_NEU_XX. Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1005_ITH_NEU_XX. Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1028_TIE_NEU_XX. Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1067_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1018_IWW_SAD_XX.wa Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1055_TSI_SAD_XX. Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1038_IEO_SAD_LO. Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1054_IOM_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-16 11:21:53.751225: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-16 11:21:53.751607: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_1 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_2 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_3 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_4 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_5 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 164s 97ms/step - loss: 2.9087 - accuracy: 0.2466 - val_loss: 2.3251 - val_accuracy: 0.2311

Epoch 00001: val_loss improved from inf to 2.32506, saving model to adadeltaweights.hdf5
Epoch 2/50
1695/1695 [==============================] - 158s 93ms/step - loss: 2.1431 - accuracy: 0.2743 - val_loss: 1.7403 - val_accuracy: 0.3491

Epoch 00002: val_loss improved from 2.32506 to 1.74033, saving model to adadeltaweights.hdf5
Epoch 3/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.9813 - accuracy: 0.3115 - val_loss: 1.5200 - val_accuracy: 0.3538

Epoch 00003: val_loss improved from 1.74033 to 1.52000, saving model to adadeltaweights.hdf5
Epoch 4/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.8953 - accuracy: 0.3121 - val_loss: 2.0477 - val_accuracy: 0.1816

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.8138 - accuracy: 0.3292 - val_loss: 1.4705 - val_accuracy: 0.4104

Epoch 00005: val_loss improved from 1.52000 to 1.47051, saving model to adadeltaweights.hdf5
Epoch 6/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.7482 - accuracy: 0.3280 - val_loss: 1.4689 - val_accuracy: 0.3844

Epoch 00006: val_loss improved from 1.47051 to 1.46892, saving model to adadeltaweights.hdf5
Epoch 7/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.7434 - accuracy: 0.3280 - val_loss: 1.4621 - val_accuracy: 0.4080

Epoch 00007: val_loss improved from 1.46892 to 1.46208, saving model to adadeltaweights.hdf5
Epoch 8/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.6819 - accuracy: 0.3611 - val_loss: 1.3924 - val_accuracy: 0.4458

Epoch 00008: val_loss improved from 1.46208 to 1.39242, saving model to adadeltaweights.hdf5
Epoch 9/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.6457 - accuracy: 0.3481 - val_loss: 1.5951 - val_accuracy: 0.3632

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6604 - accuracy: 0.3404 - val_loss: 1.4927 - val_accuracy: 0.4198

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5360 - accuracy: 0.3917 - val_loss: 1.4131 - val_accuracy: 0.4599

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 157s 92ms/step - loss: 1.5404 - accuracy: 0.3829 - val_loss: 1.4788 - val_accuracy: 0.4175

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.5126 - accuracy: 0.3876 - val_loss: 1.3854 - val_accuracy: 0.4198

Epoch 00013: val_loss improved from 1.39242 to 1.38539, saving model to adadeltaweights.hdf5
Epoch 14/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.4947 - accuracy: 0.3935 - val_loss: 1.4586 - val_accuracy: 0.4528

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.4566 - accuracy: 0.4065 - val_loss: 1.3561 - val_accuracy: 0.4505

Epoch 00015: val_loss improved from 1.38539 to 1.35614, saving model to adadeltaweights.hdf5
Epoch 16/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.4538 - accuracy: 0.4260 - val_loss: 1.4468 - val_accuracy: 0.4222

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.4208 - accuracy: 0.4271 - val_loss: 1.3805 - val_accuracy: 0.4528

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.4176 - accuracy: 0.4366 - val_loss: 1.3024 - val_accuracy: 0.4929

Epoch 00018: val_loss improved from 1.35614 to 1.30243, saving model to adadeltaweights.hdf5
Epoch 19/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.3902 - accuracy: 0.4478 - val_loss: 1.3618 - val_accuracy: 0.4599

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.3918 - accuracy: 0.4454 - val_loss: 1.3327 - val_accuracy: 0.4764

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.3734 - accuracy: 0.4643 - val_loss: 1.3022 - val_accuracy: 0.4811

Epoch 00021: val_loss improved from 1.30243 to 1.30219, saving model to adadeltaweights.hdf5
Epoch 22/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.3693 - accuracy: 0.4496 - val_loss: 1.3445 - val_accuracy: 0.4458

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.3336 - accuracy: 0.4879 - val_loss: 1.2820 - val_accuracy: 0.4906

Epoch 00023: val_loss improved from 1.30219 to 1.28202, saving model to adadeltaweights.hdf5
Epoch 24/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.3386 - accuracy: 0.4637 - val_loss: 1.2753 - val_accuracy: 0.5047

Epoch 00024: val_loss improved from 1.28202 to 1.27531, saving model to adadeltaweights.hdf5
Epoch 25/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.3282 - accuracy: 0.4791 - val_loss: 1.2715 - val_accuracy: 0.5071

Epoch 00025: val_loss improved from 1.27531 to 1.27147, saving model to adadeltaweights.hdf5
Epoch 26/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.3279 - accuracy: 0.4802 - val_loss: 1.2707 - val_accuracy: 0.4788

Epoch 00026: val_loss improved from 1.27147 to 1.27071, saving model to adadeltaweights.hdf5
Epoch 27/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.3007 - accuracy: 0.4838 - val_loss: 1.3400 - val_accuracy: 0.4788

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2870 - accuracy: 0.4991 - val_loss: 1.2856 - val_accuracy: 0.4741

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.3010 - accuracy: 0.4844 - val_loss: 1.2822 - val_accuracy: 0.4882

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2759 - accuracy: 0.4956 - val_loss: 1.3142 - val_accuracy: 0.5000

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2713 - accuracy: 0.4920 - val_loss: 1.2947 - val_accuracy: 0.5236

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2405 - accuracy: 0.5056 - val_loss: 1.2564 - val_accuracy: 0.5142

Epoch 00032: val_loss improved from 1.27071 to 1.25641, saving model to adadeltaweights.hdf5
Epoch 33/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2533 - accuracy: 0.5044 - val_loss: 1.3585 - val_accuracy: 0.4458

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2493 - accuracy: 0.4932 - val_loss: 1.2610 - val_accuracy: 0.5047

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2413 - accuracy: 0.5115 - val_loss: 1.3064 - val_accuracy: 0.4929

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2106 - accuracy: 0.5280 - val_loss: 1.3226 - val_accuracy: 0.4906

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2135 - accuracy: 0.5310 - val_loss: 1.3287 - val_accuracy: 0.4623

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2201 - accuracy: 0.5198 - val_loss: 1.2844 - val_accuracy: 0.4976

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1904 - accuracy: 0.5475 - val_loss: 1.2893 - val_accuracy: 0.4882

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2150 - accuracy: 0.5204 - val_loss: 1.3306 - val_accuracy: 0.4481

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1745 - accuracy: 0.5451 - val_loss: 1.2722 - val_accuracy: 0.4835

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1736 - accuracy: 0.5510 - val_loss: 1.2762 - val_accuracy: 0.5047

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1808 - accuracy: 0.5363 - val_loss: 1.3494 - val_accuracy: 0.4764

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1491 - accuracy: 0.5522 - val_loss: 1.4159 - val_accuracy: 0.4646

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1607 - accuracy: 0.5410 - val_loss: 1.3234 - val_accuracy: 0.4788

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.1431 - accuracy: 0.5546 - val_loss: 1.3230 - val_accuracy: 0.4670

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.1128 - accuracy: 0.5617 - val_loss: 1.3816 - val_accuracy: 0.4670

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1050 - accuracy: 0.5705 - val_loss: 1.2982 - val_accuracy: 0.4741

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1391 - accuracy: 0.5587 - val_loss: 1.3705 - val_accuracy: 0.4599

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.1115 - accuracy: 0.5676 - val_loss: 1.3081 - val_accuracy: 0.4906

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1070_ITS_ANG Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1075_IEO_ANG_HI.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1060_TSI_DIS Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1020_DFA_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1038_ITH_FEA Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1036_TAI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1081_TAI_HAP Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1057_ITS_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1089_TSI_NEU Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1063_TIE_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1015_IWW_SAD Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1079_MTI_SAD_XX.wav.npz                  
adadelta  gives the following results:
Dev set loss: 1.3268565208392036
Dev set accuracy: 0.5112782120704651
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1025_DFA_ANG_XX.wa Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1058_IEO_ANG_HI. Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1007_ITH_ANG_XX. Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1086_TAI_ANG_XX. Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1081_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1026_TIE_DIS_XX.wa Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1046_IEO_DIS_LO. Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1046_ITH_DIS_XX. Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1083_IEO_DIS_LO. Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1057_TIE_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1009_IOM_FEA_XX.wa Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1059_TIE_FEA_XX. Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1013_ITH_FEA_XX. Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1086_IWL_FEA_XX. Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1087_IOM_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1054_IEO_HAP_HI.wa Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1019_IEO_HAP_HI. Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1037_WSI_HAP_XX. Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1056_IWW_HAP_XX. Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1069_IEO_HAP_MD.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1020_IWL_NEU_XX.wa Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1057_IOM_NEU_XX. Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1022_DFA_NEU_XX. Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1070_TAI_NEU_XX. Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1050_MTI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1023_ITS_SAD_XX.wa Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1050_IEO_SAD_HI. Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1071_IWW_SAD_XX. Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1041_IWW_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
Traceback (most recent call last):
  File "experiments.py", line 84, in <module>
    runoptimizerexperiment(["adadelta", "Adam(lr = 0.00001)", "Adam(lr = 0.0001)"], "/experiments/optimizers")
  File "experiments.py", line 79, in runoptimizerexperiment
    train_network( weights_file_out = path, optimizer=l)
  File "experiments.py", line 38, in train_network
    model, serial_model = setup_model(X_train, class_names, weights_file_in=weights_file_in, optimizer=optimizer)
  File "/Users/evangelie/Documents/GitHub/panotti/panotti/models.py", line 244, in setup_model
    serial_model.compile(loss=loss, optimizer=opt, metrics=metrics)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 75, in symbolic_fn_wrapper
    return func(*args, **kwargs)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py", line 95, in compile
    self.optimizer = optimizers.get(optimizer)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py", line 868, in get
    return deserialize(config)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py", line 833, in deserialize
    printable_module_name='optimizer')
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/generic_utils.py", line 140, in deserialize_keras_object
    ': ' + class_name)
ValueError: Unknown optimizer: Adam(lr = 0.00001)
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1073_IEO_ANG_MD.wa Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1065_IEO_ANG_MD. Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1021_IWL_ANG_XX. Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1026_IWW_ANG_XX. Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1026_ITS_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1040_DFA_DIS_XX.wa Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1007_IWL_DIS_XX. Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1055_IEO_DIS_MD. Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1040_IWW_DIS_XX. Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1088_TSI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1062_IWL_FEA_XX.wa Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1066_ITH_FEA_XX. Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1012_IEO_FEA_HI. Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1014_TIE_FEA_XX. Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1075_IOM_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1024_IEO_HAP_HI.wa Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1048_IWW_HAP_XX. Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1026_TSI_HAP_XX. Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1005_WSI_HAP_XX. Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1053_IOM_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1087_TIE_NEU_XX.wa Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1037_ITH_NEU_XX. Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1039_IEO_NEU_XX. Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1027_ITS_NEU_XX. Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1010_MTI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1058_IOM_SAD_XX.wa Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1091_IEO_SAD_LO. Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1018_TSI_SAD_XX. Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1022_ITS_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-16 14:53:26.409703: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-16 14:53:26.410050: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
Traceback (most recent call last):
  File "experiments.py", line 84, in <module>
    runoptimizerexperiment([ "Adam(learning_rate = 0.00001)", "Adam(learning_rate = 0.0001)"], "/experiments/optimizers")#"adadelta",
  File "experiments.py", line 79, in runoptimizerexperiment
    train_network( weights_file_out = path, optimizer=l)
  File "experiments.py", line 38, in train_network
    model, serial_model = setup_model(X_train, class_names, weights_file_in=weights_file_in, optimizer=optimizer)
  File "/Users/evangelie/Documents/GitHub/panotti/panotti/models.py", line 244, in setup_model
    serial_model.compile(loss=loss, optimizer=opt, metrics=metrics)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py", line 75, in symbolic_fn_wrapper
    return func(*args, **kwargs)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py", line 95, in compile
    self.optimizer = optimizers.get(optimizer)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py", line 868, in get
    return deserialize(config)
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/optimizers.py", line 833, in deserialize
    printable_module_name='optimizer')
  File "/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/generic_utils.py", line 140, in deserialize_keras_object
    ': ' + class_name)
ValueError: Unknown optimizer: Adam(learning_rate = 0.00001)
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
Traceback (most recent call last):
  File "experiments.py", line 85, in <module>
    runoptimizerexperiment(["Adam", "adadelta"], [1, 0.001,0.00001],)#([ "Adam(learning_rate = 0.00001)", "Adam(learning_rate = 0.0001)"], "/experiments/optimizers")#"adadelta",
TypeError: runoptimizerexperiment() missing 1 required positional argument: 'directory'
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
Traceback (most recent call last):
  File "experiments.py", line 85, in <module>
    runoptimizerexperiment(["Adam", "adadelta"], [1, 0.001,0.00001],"")#([ "Adam(learning_rate = 0.00001)", "Adam(learning_rate = 0.0001)"], "/experiments/optimizers")#"adadelta",
  File "experiments.py", line 79, in runoptimizerexperiment
    path = list[i] + str(r) + "weights.hdf5"
NameError: name 'i' is not defined
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1091_TSI_ANG_XX.wav.npz         Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1020_DFA_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1032_DFA_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1080_IWW_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1053_IWW_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1017_ITS_DIS_XX.wav.npz         Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1054_TSI_DIS_XX.wav.npz       Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1005_MTI_DIS_XX.wav.npz       Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1091_IEO_DIS_LO.wav.npz       Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1038_ITH_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1078_ITH_FEA_XX.wav.npz         Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1021_MTI_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1025_MTI_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1087_IWW_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1029_TIE_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1047_MTI_HAP_XX.wav.npz         Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1038_IOM_HAP_XX.wav.npz       Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1077_IWL_HAP_XX.wav.npz       Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1074_TAI_HAP_XX.wav.npz       Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1023_IEO_HAP_HI.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1087_IOM_NEU_XX.wav.npz         Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1014_WSI_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1084_MTI_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1017_ITS_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1015_TIE_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1050_IEO_SAD_HI.wav.npz         Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1019_TSI_SAD_XX.wav.npz       Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1032_DFA_SAD_XX.wav.npz       Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1011_TIE_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-16 16:50:24.620797: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-16 16:50:24.621134: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
Traceback (most recent call last):
  File "experiments.py", line 85, in <module>
    runoptimizerexperiment(["Adam", "adadelta"], [1, 0.001,0.00001],"")#([ "Adam(learning_rate = 0.00001)", "Adam(learning_rate = 0.0001)"], "/experiments/optimizers")#"adadelta",
  File "experiments.py", line 80, in runoptimizerexperiment
    train_network( weights_file_out = path, optimizer=l, learningrate = r)
  File "experiments.py", line 38, in train_network
    model, serial_model = setup_model(X_train, class_names, weights_file_in=weights_file_in, optimizer=optimizer, lr = learningrate)
  File "/Users/evangelie/Documents/GitHub/panotti/panotti/models.py", line 235, in setup_model
    opt = optimizer(learning_rate = lr)#'adadelta' # Adam(lr = 0.00001)  # So far, adadelta seems to work the best of things I've tried
TypeError: 'str' object is not callable
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1024_DFA_ANG_XX.wav.npz         Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1064_IWL_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1002_TIE_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1089_ITS_ANG_XX.wav.npz       Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1062_IWW_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1026_IWW_DIS_XX.wav.npz         Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1020_IEO_DIS_LO.wav.npz       Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1081_TSI_DIS_XX.wav.npz       Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1020_ITS_DIS_XX.wav.npz       Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1028_TAI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1091_MTI_FEA_XX.wav.npz         Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1039_DFA_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1042_TIE_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1049_TIE_FEA_XX.wav.npz       Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1071_TAI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1058_ITH_HAP_XX.wav.npz         Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1059_IOM_HAP_XX.wav.npz       Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1077_IEO_HAP_LO.wav.npz       Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1076_ITS_HAP_XX.wav.npz       Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1043_TAI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1036_TAI_NEU_XX.wav.npz         Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1049_IWL_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1091_IWL_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1001_ITH_NEU_XX.wav.npz       Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1066_DFA_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1001_IEO_SAD_HI.wav.npz         Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1089_TSI_SAD_XX.wav.npz       Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1015_WSI_SAD_XX.wav.npz       Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1049_IEO_SAD_LO.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-16 16:53:06.114336: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-16 16:53:06.114651: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_1 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_2 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_3 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_4 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_5 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 162s 96ms/step - loss: 934389899.6427 - accuracy: 0.1717 - val_loss: 193.3048 - val_accuracy: 0.1580

Epoch 00001: val_loss improved from inf to 193.30477, saving model to Adam1weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 176s 104ms/step - loss: 265729184.3536 - accuracy: 0.1540 - val_loss: 193.9717 - val_accuracy: 0.1533

Epoch 00002: val_loss did not improve
Epoch 3/50
1695/1695 [==============================] - 161s 95ms/step - loss: 41835775.5699 - accuracy: 0.1646 - val_loss: 169.2603 - val_accuracy: 0.1840

Epoch 00003: val_loss improved from 193.30477 to 169.26027, saving model to Adam1weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 155s 92ms/step - loss: 232.8298 - accuracy: 0.1788 - val_loss: 119.0272 - val_accuracy: 0.1533

Epoch 00004: val_loss improved from 169.26027 to 119.02721, saving model to Adam1weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 155s 92ms/step - loss: 344042026.3894 - accuracy: 0.1811 - val_loss: 134.5777 - val_accuracy: 0.1840

Epoch 00005: val_loss did not improve
Epoch 6/50
1695/1695 [==============================] - 155s 91ms/step - loss: 7724236.5854 - accuracy: 0.1493 - val_loss: 91.1470 - val_accuracy: 0.1533

Epoch 00006: val_loss improved from 119.02721 to 91.14702, saving model to Adam1weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 154s 91ms/step - loss: 249.4739 - accuracy: 0.1705 - val_loss: 74.3383 - val_accuracy: 0.1533

Epoch 00007: val_loss improved from 91.14702 to 74.33834, saving model to Adam1weights.hdf5
Epoch 8/50
1695/1695 [==============================] - 155s 91ms/step - loss: 186.2343 - accuracy: 0.1558 - val_loss: 55.7367 - val_accuracy: 0.1533

Epoch 00008: val_loss improved from 74.33834 to 55.73674, saving model to Adam1weights.hdf5
Epoch 9/50
1695/1695 [==============================] - 155s 91ms/step - loss: 5717642.0646 - accuracy: 0.1717 - val_loss: 39.7807 - val_accuracy: 0.1816

Epoch 00009: val_loss improved from 55.73674 to 39.78072, saving model to Adam1weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 155s 91ms/step - loss: 170.6751 - accuracy: 0.1552 - val_loss: 33.6469 - val_accuracy: 0.1816

Epoch 00010: val_loss improved from 39.78072 to 33.64688, saving model to Adam1weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 156s 92ms/step - loss: 3154614751.9558 - accuracy: 0.1646 - val_loss: 146525.5043 - val_accuracy: 0.1840

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 155s 91ms/step - loss: 45266942960.7304 - accuracy: 0.1611 - val_loss: 1757.4341 - val_accuracy: 0.1580

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 155s 92ms/step - loss: 45074442201.9897 - accuracy: 0.1628 - val_loss: 1567.7262 - val_accuracy: 0.1580

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1502.7186 - accuracy: 0.1611 - val_loss: 1387.0696 - val_accuracy: 0.1580

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 165s 98ms/step - loss: 1326.6023 - accuracy: 0.1546 - val_loss: 1238.5975 - val_accuracy: 0.1580

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1209.7878 - accuracy: 0.1599 - val_loss: 1108.6204 - val_accuracy: 0.1580

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 160s 95ms/step - loss: 1091.9691 - accuracy: 0.1569 - val_loss: 986.5550 - val_accuracy: 0.1580

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 158s 93ms/step - loss: 6046463290.5254 - accuracy: 0.1587 - val_loss: 914.9322 - val_accuracy: 0.1580

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 155s 91ms/step - loss: 27662203113.4749 - accuracy: 0.1593 - val_loss: 729.4915 - val_accuracy: 0.1580

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 155s 91ms/step - loss: 811.7549 - accuracy: 0.1658 - val_loss: 633.7701 - val_accuracy: 0.1580

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 155s 91ms/step - loss: 191804087398.3077 - accuracy: 0.1522 - val_loss: 485.4334 - val_accuracy: 0.1533

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 155s 91ms/step - loss: 163052789365.2232 - accuracy: 0.1487 - val_loss: 959.7553 - val_accuracy: 0.1392

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 155s 91ms/step - loss: 43710506530.8708 - accuracy: 0.1705 - val_loss: 1072.6142 - val_accuracy: 0.1392

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 156s 92ms/step - loss: 480253332320.6330 - accuracy: 0.1835 - val_loss: 633.4318 - val_accuracy: 0.1840

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 174s 103ms/step - loss: 868.6362 - accuracy: 0.1611 - val_loss: 473.0242 - val_accuracy: 0.1816

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 204s 120ms/step - loss: 258975630983.1359 - accuracy: 0.1605 - val_loss: 907.8499 - val_accuracy: 0.1816

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 227s 134ms/step - loss: 14068565237.1524 - accuracy: 0.1693 - val_loss: 1742.8008 - val_accuracy: 0.1816

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 205s 121ms/step - loss: 691466416.4605 - accuracy: 0.1646 - val_loss: 1560.3290 - val_accuracy: 0.1816

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 222s 131ms/step - loss: 1527.5804 - accuracy: 0.1711 - val_loss: 1386.2461 - val_accuracy: 0.1816

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 243s 143ms/step - loss: 1350.7229 - accuracy: 0.1676 - val_loss: 1237.5028 - val_accuracy: 0.1816

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 249s 147ms/step - loss: 1276.7170 - accuracy: 0.1605 - val_loss: 1102.2820 - val_accuracy: 0.1816

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 232s 137ms/step - loss: 175042109308.5179 - accuracy: 0.1593 - val_loss: 1051.8247 - val_accuracy: 0.1816

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 205s 121ms/step - loss: 27801456541.1396 - accuracy: 0.1693 - val_loss: 932.8450 - val_accuracy: 0.1816

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 202s 119ms/step - loss: 1124.9993 - accuracy: 0.1770 - val_loss: 852.1596 - val_accuracy: 0.1816

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 209s 123ms/step - loss: 20008787610.2303 - accuracy: 0.1788 - val_loss: 805.5865 - val_accuracy: 0.1816

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 208s 123ms/step - loss: 1177.6853 - accuracy: 0.1799 - val_loss: 765.5617 - val_accuracy: 0.1816

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 200s 118ms/step - loss: 1147.1285 - accuracy: 0.1693 - val_loss: 716.9443 - val_accuracy: 0.1816

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 201s 119ms/step - loss: 200294079.7923 - accuracy: 0.1805 - val_loss: 650.8744 - val_accuracy: 0.1816

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 197s 116ms/step - loss: 174451333472.9990 - accuracy: 0.1640 - val_loss: 871.3661 - val_accuracy: 0.1840

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 168s 99ms/step - loss: 7224957594.7650 - accuracy: 0.1676 - val_loss: 1279.2001 - val_accuracy: 0.1840

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 172s 101ms/step - loss: 195596.9728 - accuracy: 0.1664 - val_loss: 1090.2949 - val_accuracy: 0.1840

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 168s 99ms/step - loss: 120629605.1946 - accuracy: 0.1811 - val_loss: 930.4715 - val_accuracy: 0.1840

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 157s 93ms/step - loss: 69152258328.2743 - accuracy: 0.1729 - val_loss: 764.9533 - val_accuracy: 0.1392

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 182s 107ms/step - loss: 1020.0405 - accuracy: 0.1646 - val_loss: 639.0643 - val_accuracy: 0.1392

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 173s 102ms/step - loss: 9723.9490 - accuracy: 0.1628 - val_loss: 536.6785 - val_accuracy: 0.1392

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 192s 113ms/step - loss: 914.9193 - accuracy: 0.1699 - val_loss: 451.0241 - val_accuracy: 0.1392

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 169s 100ms/step - loss: 859.0515 - accuracy: 0.1717 - val_loss: 381.5696 - val_accuracy: 0.1392

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 171s 101ms/step - loss: 842.5547 - accuracy: 0.1658 - val_loss: 323.8776 - val_accuracy: 0.1392

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 177s 104ms/step - loss: 815.3407 - accuracy: 0.1794 - val_loss: 273.4872 - val_accuracy: 0.1816

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 171s 101ms/step - loss: 782.0466 - accuracy: 0.1622 - val_loss: 237.5863 - val_accuracy: 0.1816

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1089_WSI_ANG_XX.wav.npz           Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1051_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1030_WSI_DIS_XX.wav.npz           Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1037_ITS_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1069_IWW_FEA_XX.wav.npz           Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1060_IEO_FEA_LO.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1045_TSI_HAP_XX.wav.npz           Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1058_WSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1075_MTI_NEU_XX.wav.npz           Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1091_ITH_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1040_IEO_SAD_LO.wav.npz           Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1055_MTI_SAD_XX.wav.npz                  
Adam  gives the following results:
Dev set loss: 217.19786588052162
Dev set accuracy: 0.17669172585010529
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1069_ITS_ANG_XX.wav.npz                 Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1007_IWL_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1073_TSI_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1006_IEO_ANG_MD.wav.npz               Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1056_IEO_ANG_MD.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1017_IEO_DIS_MD.wav.npz                 Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1036_TIE_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1035_IEO_DIS_MD.wav.npz               Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1033_MTI_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1020_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1080_IWW_FEA_XX.wav.npz                 Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1053_TIE_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1025_DFA_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1006_IWW_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1056_IWW_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1080_IEO_HAP_HI.wav.npz                 Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1048_IWL_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1017_TAI_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1073_IWW_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1085_ITH_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1061_IWL_NEU_XX.wav.npz                 Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1081_ITH_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1051_IWW_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1063_MTI_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1080_TAI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1073_MTI_SAD_XX.wav.npz                 Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1066_ITH_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1073_TSI_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1087_ITS_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_6 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_7 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_8 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_9 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_10 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 173s 102ms/step - loss: 2.7667 - accuracy: 0.2614 - val_loss: 1.7123 - val_accuracy: 0.3090

Epoch 00001: val_loss improved from inf to 1.71231, saving model to Adam0.001weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 175s 103ms/step - loss: 2.0653 - accuracy: 0.2891 - val_loss: 2.2681 - val_accuracy: 0.3019

Epoch 00002: val_loss did not improve
Epoch 3/50
1695/1695 [==============================] - 181s 107ms/step - loss: 1.9609 - accuracy: 0.2938 - val_loss: 1.6780 - val_accuracy: 0.3160

Epoch 00003: val_loss improved from 1.71231 to 1.67796, saving model to Adam0.001weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.9308 - accuracy: 0.2903 - val_loss: 1.5917 - val_accuracy: 0.3137

Epoch 00004: val_loss improved from 1.67796 to 1.59169, saving model to Adam0.001weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 172s 101ms/step - loss: 1.7505 - accuracy: 0.3428 - val_loss: 1.8327 - val_accuracy: 0.3019

Epoch 00005: val_loss did not improve
Epoch 6/50
1695/1695 [==============================] - 184s 109ms/step - loss: 1.6994 - accuracy: 0.3563 - val_loss: 1.4916 - val_accuracy: 0.3396

Epoch 00006: val_loss improved from 1.59169 to 1.49162, saving model to Adam0.001weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 180s 106ms/step - loss: 1.6397 - accuracy: 0.3528 - val_loss: 1.5865 - val_accuracy: 0.3915

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 171s 101ms/step - loss: 1.6057 - accuracy: 0.3611 - val_loss: 1.6197 - val_accuracy: 0.3703

Epoch 00008: val_loss did not improve
Epoch 9/50
1695/1695 [==============================] - 173s 102ms/step - loss: 1.5612 - accuracy: 0.3740 - val_loss: 1.5119 - val_accuracy: 0.4552

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 182s 108ms/step - loss: 1.5294 - accuracy: 0.4047 - val_loss: 1.4765 - val_accuracy: 0.3750

Epoch 00010: val_loss improved from 1.49162 to 1.47653, saving model to Adam0.001weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 179s 105ms/step - loss: 1.4976 - accuracy: 0.4159 - val_loss: 1.4079 - val_accuracy: 0.4292

Epoch 00011: val_loss improved from 1.47653 to 1.40793, saving model to Adam0.001weights.hdf5
Epoch 12/50
1695/1695 [==============================] - 176s 104ms/step - loss: 1.4990 - accuracy: 0.3976 - val_loss: 1.4795 - val_accuracy: 0.4269

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 173s 102ms/step - loss: 1.4536 - accuracy: 0.4307 - val_loss: 1.4559 - val_accuracy: 0.4410

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 176s 104ms/step - loss: 1.4178 - accuracy: 0.4212 - val_loss: 1.4363 - val_accuracy: 0.4080

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 177s 104ms/step - loss: 1.4311 - accuracy: 0.4242 - val_loss: 1.5179 - val_accuracy: 0.3962

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 172s 102ms/step - loss: 1.4180 - accuracy: 0.4236 - val_loss: 1.5451 - val_accuracy: 0.4175

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.3822 - accuracy: 0.4501 - val_loss: 1.3831 - val_accuracy: 0.4505

Epoch 00017: val_loss improved from 1.40793 to 1.38314, saving model to Adam0.001weights.hdf5
Epoch 18/50
1695/1695 [==============================] - 176s 104ms/step - loss: 1.3915 - accuracy: 0.4401 - val_loss: 1.4330 - val_accuracy: 0.4269

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 175s 103ms/step - loss: 1.3489 - accuracy: 0.4407 - val_loss: 1.5345 - val_accuracy: 0.4175

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.3601 - accuracy: 0.4472 - val_loss: 1.4342 - val_accuracy: 0.4363

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 182s 107ms/step - loss: 1.3423 - accuracy: 0.4566 - val_loss: 1.4002 - val_accuracy: 0.4552

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 200s 118ms/step - loss: 1.3342 - accuracy: 0.4755 - val_loss: 1.3936 - val_accuracy: 0.4575

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 170s 101ms/step - loss: 1.3292 - accuracy: 0.4684 - val_loss: 1.5369 - val_accuracy: 0.3868

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.3208 - accuracy: 0.4608 - val_loss: 1.3493 - val_accuracy: 0.4575

Epoch 00024: val_loss improved from 1.38314 to 1.34926, saving model to Adam0.001weights.hdf5
Epoch 25/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.3068 - accuracy: 0.4608 - val_loss: 1.4014 - val_accuracy: 0.4646

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 159s 94ms/step - loss: 1.3043 - accuracy: 0.4737 - val_loss: 1.3494 - val_accuracy: 0.4788

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 165s 97ms/step - loss: 1.3008 - accuracy: 0.4855 - val_loss: 1.3556 - val_accuracy: 0.4741

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 167s 99ms/step - loss: 1.2906 - accuracy: 0.4767 - val_loss: 1.3418 - val_accuracy: 0.4953

Epoch 00028: val_loss improved from 1.34926 to 1.34182, saving model to Adam0.001weights.hdf5
Epoch 29/50
1695/1695 [==============================] - 160s 94ms/step - loss: 1.2426 - accuracy: 0.4956 - val_loss: 1.3924 - val_accuracy: 0.4410

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.2967 - accuracy: 0.4850 - val_loss: 1.3781 - val_accuracy: 0.4599

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.2388 - accuracy: 0.4979 - val_loss: 1.3857 - val_accuracy: 0.4552

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2384 - accuracy: 0.5003 - val_loss: 1.3935 - val_accuracy: 0.4717

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2600 - accuracy: 0.4932 - val_loss: 1.4651 - val_accuracy: 0.4741

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.2504 - accuracy: 0.4944 - val_loss: 1.3264 - val_accuracy: 0.4717

Epoch 00034: val_loss improved from 1.34182 to 1.32639, saving model to Adam0.001weights.hdf5
Epoch 35/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2487 - accuracy: 0.5127 - val_loss: 1.3788 - val_accuracy: 0.4835

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.2382 - accuracy: 0.5115 - val_loss: 1.3556 - val_accuracy: 0.4458

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.2098 - accuracy: 0.5074 - val_loss: 1.4260 - val_accuracy: 0.4599

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.2154 - accuracy: 0.5115 - val_loss: 1.4117 - val_accuracy: 0.4599

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1708 - accuracy: 0.5263 - val_loss: 1.3603 - val_accuracy: 0.4858

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.2238 - accuracy: 0.5103 - val_loss: 1.3512 - val_accuracy: 0.4717

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1714 - accuracy: 0.5316 - val_loss: 1.5602 - val_accuracy: 0.4151

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.1961 - accuracy: 0.5316 - val_loss: 1.3573 - val_accuracy: 0.4906

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1822 - accuracy: 0.5440 - val_loss: 1.4559 - val_accuracy: 0.4363

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1988 - accuracy: 0.5263 - val_loss: 1.4449 - val_accuracy: 0.4552

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 151s 89ms/step - loss: 1.1612 - accuracy: 0.5457 - val_loss: 1.3607 - val_accuracy: 0.4788

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1475 - accuracy: 0.5469 - val_loss: 1.3858 - val_accuracy: 0.4764

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.1514 - accuracy: 0.5540 - val_loss: 1.4510 - val_accuracy: 0.4575

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.1332 - accuracy: 0.5469 - val_loss: 1.3778 - val_accuracy: 0.4882

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1557 - accuracy: 0.5510 - val_loss: 1.5007 - val_accuracy: 0.4552

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.1092 - accuracy: 0.5699 - val_loss: 1.3811 - val_accuracy: 0.4953

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1013_TSI_ANG_XX.wav.npz           Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1023_IOM_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1053_IWW_DIS_XX.wav.npz           Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1062_IEO_DIS_HI.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1076_TIE_FEA_XX.wav.npz           Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1043_DFA_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1069_ITS_HAP_XX.wav.npz           Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1058_IWL_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1024_WSI_NEU_XX.wav.npz           Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1042_TSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1037_IEO_SAD_LO.wav.npz           Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1015_IWW_SAD_XX.wav.npz                  
Adam  gives the following results:
Dev set loss: 1.4017318514056671
Dev set accuracy: 0.4661654233932495
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1071_IOM_ANG_XX.wav.npz                 Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1012_TAI_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1006_IEO_ANG_MD.wav.npz               Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1036_IEO_ANG_LO.wav.npz               Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1034_IOM_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1044_MTI_DIS_XX.wav.npz                 Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1023_IEO_DIS_LO.wav.npz               Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1069_ITS_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1031_TAI_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1033_MTI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1073_MTI_FEA_XX.wav.npz                 Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1060_ITS_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1020_IOM_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1054_IEO_FEA_MD.wav.npz               Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1066_TAI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1037_TIE_HAP_XX.wav.npz                 Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1020_TIE_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1086_MTI_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1083_DFA_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1027_IEO_HAP_HI.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1021_IWL_NEU_XX.wav.npz                 Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1031_IEO_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1009_TIE_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1060_IOM_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1031_IWW_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1013_IEO_SAD_LO.wav.npz                 Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1050_WSI_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1048_IEO_SAD_LO.wav.npz               Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1014_ITH_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_11 (Activation)   (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 24, 105, 32)       0         
_________________________________________________________________
activation_12 (Activation)   (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 12, 52, 32)        0         
_________________________________________________________________
activation_13 (Activation)   (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 6, 26, 32)         0         
_________________________________________________________________
activation_14 (Activation)   (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_3 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_15 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 128)               0         
_________________________________________________________________
dense_6 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 155s 92ms/step - loss: 5.9701 - accuracy: 0.1628 - val_loss: 1.7240 - val_accuracy: 0.2712

Epoch 00001: val_loss improved from inf to 1.72401, saving model to Adam1e-05weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 152s 89ms/step - loss: 4.4946 - accuracy: 0.2035 - val_loss: 1.6545 - val_accuracy: 0.3208

Epoch 00002: val_loss improved from 1.72401 to 1.65450, saving model to Adam1e-05weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 151s 89ms/step - loss: 3.8733 - accuracy: 0.2106 - val_loss: 1.6168 - val_accuracy: 0.3396

Epoch 00003: val_loss improved from 1.65450 to 1.61683, saving model to Adam1e-05weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 151s 89ms/step - loss: 3.5303 - accuracy: 0.2142 - val_loss: 1.5950 - val_accuracy: 0.3467

Epoch 00004: val_loss improved from 1.61683 to 1.59497, saving model to Adam1e-05weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 152s 89ms/step - loss: 3.1385 - accuracy: 0.2372 - val_loss: 1.5730 - val_accuracy: 0.3656

Epoch 00005: val_loss improved from 1.59497 to 1.57297, saving model to Adam1e-05weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 151s 89ms/step - loss: 3.1021 - accuracy: 0.2248 - val_loss: 1.5717 - val_accuracy: 0.3797

Epoch 00006: val_loss improved from 1.57297 to 1.57173, saving model to Adam1e-05weights.hdf5
Epoch 7/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.9650 - accuracy: 0.2277 - val_loss: 1.5768 - val_accuracy: 0.3774

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.6768 - accuracy: 0.2407 - val_loss: 1.5808 - val_accuracy: 0.3726

Epoch 00008: val_loss did not improve
Epoch 9/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.7812 - accuracy: 0.2301 - val_loss: 1.5831 - val_accuracy: 0.3844

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.6596 - accuracy: 0.2507 - val_loss: 1.5884 - val_accuracy: 0.3892

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.5439 - accuracy: 0.2466 - val_loss: 1.6007 - val_accuracy: 0.3868

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.6002 - accuracy: 0.2142 - val_loss: 1.6007 - val_accuracy: 0.3844

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.5158 - accuracy: 0.2466 - val_loss: 1.6062 - val_accuracy: 0.3797

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.4770 - accuracy: 0.2454 - val_loss: 1.6087 - val_accuracy: 0.3774

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.4957 - accuracy: 0.2407 - val_loss: 1.6098 - val_accuracy: 0.3726

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.4531 - accuracy: 0.2448 - val_loss: 1.6053 - val_accuracy: 0.3774

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.3927 - accuracy: 0.2631 - val_loss: 1.6104 - val_accuracy: 0.3915

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.3132 - accuracy: 0.2761 - val_loss: 1.6246 - val_accuracy: 0.3821

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.3362 - accuracy: 0.2643 - val_loss: 1.6349 - val_accuracy: 0.3986

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.3102 - accuracy: 0.2802 - val_loss: 1.6402 - val_accuracy: 0.4009

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.3665 - accuracy: 0.2590 - val_loss: 1.6457 - val_accuracy: 0.3986

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.4136 - accuracy: 0.2513 - val_loss: 1.6433 - val_accuracy: 0.4009

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.3056 - accuracy: 0.2525 - val_loss: 1.6292 - val_accuracy: 0.3986

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.3044 - accuracy: 0.2549 - val_loss: 1.6353 - val_accuracy: 0.3986

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.2700 - accuracy: 0.2596 - val_loss: 1.6398 - val_accuracy: 0.3915

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.3156 - accuracy: 0.2625 - val_loss: 1.6225 - val_accuracy: 0.3892

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.2444 - accuracy: 0.2832 - val_loss: 1.6170 - val_accuracy: 0.3962

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.2808 - accuracy: 0.2543 - val_loss: 1.6322 - val_accuracy: 0.3939

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.2559 - accuracy: 0.2684 - val_loss: 1.6311 - val_accuracy: 0.3962

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.2320 - accuracy: 0.2661 - val_loss: 1.6164 - val_accuracy: 0.3962

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.2386 - accuracy: 0.2678 - val_loss: 1.6022 - val_accuracy: 0.3986

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.1485 - accuracy: 0.2761 - val_loss: 1.5966 - val_accuracy: 0.4057

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.1283 - accuracy: 0.2749 - val_loss: 1.5909 - val_accuracy: 0.4057

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.1392 - accuracy: 0.2708 - val_loss: 1.6049 - val_accuracy: 0.4009

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.1454 - accuracy: 0.2796 - val_loss: 1.6041 - val_accuracy: 0.3962

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.1566 - accuracy: 0.2844 - val_loss: 1.6034 - val_accuracy: 0.4057

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.1176 - accuracy: 0.2796 - val_loss: 1.6026 - val_accuracy: 0.3986

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.1054 - accuracy: 0.2985 - val_loss: 1.5755 - val_accuracy: 0.3939

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.1041 - accuracy: 0.2891 - val_loss: 1.5801 - val_accuracy: 0.3986

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 151s 89ms/step - loss: 2.1232 - accuracy: 0.2737 - val_loss: 1.5730 - val_accuracy: 0.4009

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0681 - accuracy: 0.2855 - val_loss: 1.5855 - val_accuracy: 0.3892

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0697 - accuracy: 0.2885 - val_loss: 1.5907 - val_accuracy: 0.3915

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0295 - accuracy: 0.3080 - val_loss: 1.5775 - val_accuracy: 0.3868

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0559 - accuracy: 0.2867 - val_loss: 1.5880 - val_accuracy: 0.3962

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0398 - accuracy: 0.2903 - val_loss: 1.5775 - val_accuracy: 0.3939

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 154s 91ms/step - loss: 2.0254 - accuracy: 0.2932 - val_loss: 1.5586 - val_accuracy: 0.3962

Epoch 00046: val_loss improved from 1.57173 to 1.55858, saving model to Adam1e-05weights.hdf5
Epoch 47/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0504 - accuracy: 0.2820 - val_loss: 1.5634 - val_accuracy: 0.3939

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0149 - accuracy: 0.2903 - val_loss: 1.5577 - val_accuracy: 0.3986

Epoch 00048: val_loss improved from 1.55858 to 1.55770, saving model to Adam1e-05weights.hdf5
Epoch 49/50
1695/1695 [==============================] - 152s 89ms/step - loss: 2.0161 - accuracy: 0.2885 - val_loss: 1.5607 - val_accuracy: 0.3892

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 152s 89ms/step - loss: 1.9873 - accuracy: 0.2973 - val_loss: 1.5582 - val_accuracy: 0.3939

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1085_IWW_ANG_XX.wav.npz           Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1082_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1079_IEO_DIS_LO.wav.npz           Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1088_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1055_IWL_FEA_XX.wav.npz           Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1007_WSI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1085_TSI_HAP_XX.wav.npz           Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1075_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1037_DFA_NEU_XX.wav.npz           Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1024_WSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1019_MTI_SAD_XX.wav.npz           Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1002_DFA_SAD_XX.wav.npz                  
Adam  gives the following results:
Dev set loss: 1.5864844098126978
Dev set accuracy: 0.3458646535873413
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1083_IWW_ANG_XX.wav.npz                 Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1076_TAI_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1085_IWL_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1079_IWL_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1034_IOM_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1073_TIE_DIS_XX.wav.npz                 Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1073_IEO_DIS_MD.wav.npz               Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1044_MTI_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1028_ITH_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1089_ITS_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1037_WSI_FEA_XX.wav.npz                 Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1062_TAI_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1015_IWL_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1069_IOM_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1056_DFA_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1046_WSI_HAP_XX.wav.npz                 Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1060_ITS_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1049_TAI_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1029_IWL_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1037_IWL_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1039_IOM_NEU_XX.wav.npz                 Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1027_WSI_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1084_MTI_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1045_ITH_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1039_IEO_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1005_IEO_SAD_LO.wav.npz                 Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1040_IWL_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1042_DFA_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1075_TIE_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
Traceback (most recent call last):
  File "experiments.py", line 85, in <module>
    runoptimizerexperiment(["Adam", "adadelta"], [1, 0.001,0.00001],"")#([ "Adam(learning_rate = 0.00001)", "Adam(learning_rate = 0.0001)"], "/experiments/optimizers")#"adadelta",
  File "experiments.py", line 80, in runoptimizerexperiment
    train_network( weights_file_out = path, optimizer=l, learningrate = r)
  File "experiments.py", line 38, in train_network
    model, serial_model = setup_model(X_train, class_names, weights_file_in=weights_file_in, optimizer=optimizer, lr = learningrate)
  File "/Users/evangelie/Documents/GitHub/panotti/panotti/models.py", line 235, in setup_model
    opt = eval(optimizer)(learning_rate = lr)#'adadelta' # Adam(lr = 0.00001)  # So far, adadelta seems to work the best of things I've tried
  File "<string>", line 1, in <module>
NameError: name 'adadelta' is not defined
(tensorflow) DN0a241607:panotti evangelie$ python experiments.py
['/Users/evangelie/Documents/GitHub/panotti', '/miniconda3/envs/tensorflow/lib/python37.zip', '/miniconda3/envs/tensorflow/lib/python3.7', '/miniconda3/envs/tensorflow/lib/python3.7/lib-dynload', '/Users/evangelie/.local/lib/python3.7/site-packages', '/miniconda3/envs/tensorflow/lib/python3.7/site-packages']
3.7.5 (default, Oct 25 2019, 10:52:18) 
[Clang 4.0.1 (tags/RELEASE_401/final)]
Using TensorFlow backend.
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1088_TSI_ANG_XX.wav.npz                 Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1020_ITS_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1085_IOM_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1085_TIE_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1077_IWW_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1075_DFA_DIS_XX.wav.npz                 Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1070_WSI_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1039_IOM_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1091_IEO_DIS_LO.wav.npz               Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1027_TSI_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1010_IEO_FEA_LO.wav.npz                 Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1036_DFA_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1066_MTI_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1026_IWL_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1066_TAI_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1029_DFA_HAP_XX.wav.npz                 Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1089_IOM_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1056_TIE_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1083_IEO_HAP_HI.wav.npz               Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1058_IOM_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1012_IWW_NEU_XX.wav.npz                 Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1028_DFA_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1075_IOM_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1079_ITS_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1072_TAI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1035_IWL_SAD_XX.wav.npz                 Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1036_DFA_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1071_WSI_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1032_IEO_SAD_MD.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
2019-11-17 02:37:02.256858: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-17 02:37:02.257224: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_1 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_2 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_3 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_4 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_5 (Activation)    (None, 128)               0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 158s 93ms/step - loss: 8.9866 - accuracy: 0.1522 - val_loss: 2.0029 - val_accuracy: 0.1816

Epoch 00001: val_loss improved from inf to 2.00289, saving model to Adam0.01weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.1923 - accuracy: 0.1794 - val_loss: 1.8411 - val_accuracy: 0.1580

Epoch 00002: val_loss improved from 2.00289 to 1.84111, saving model to Adam0.01weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.1248 - accuracy: 0.1581 - val_loss: 2.3317 - val_accuracy: 0.1792

Epoch 00003: val_loss did not improve
Epoch 4/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0170 - accuracy: 0.1711 - val_loss: 1.9104 - val_accuracy: 0.1816

Epoch 00004: val_loss did not improve
Epoch 5/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0244 - accuracy: 0.1534 - val_loss: 1.8231 - val_accuracy: 0.1840

Epoch 00005: val_loss improved from 1.84111 to 1.82314, saving model to Adam0.01weights.hdf5
Epoch 6/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9372 - accuracy: 0.1752 - val_loss: 2.0229 - val_accuracy: 0.1392

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 154s 91ms/step - loss: 1.9499 - accuracy: 0.1735 - val_loss: 1.9049 - val_accuracy: 0.1439

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9823 - accuracy: 0.1628 - val_loss: 1.9141 - val_accuracy: 0.1840

Epoch 00008: val_loss did not improve
Epoch 9/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9458 - accuracy: 0.1794 - val_loss: 1.8220 - val_accuracy: 0.1580

Epoch 00009: val_loss improved from 1.82314 to 1.82198, saving model to Adam0.01weights.hdf5
Epoch 10/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.9692 - accuracy: 0.1640 - val_loss: 1.7894 - val_accuracy: 0.1840

Epoch 00010: val_loss improved from 1.82198 to 1.78939, saving model to Adam0.01weights.hdf5
Epoch 11/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9309 - accuracy: 0.1593 - val_loss: 1.8410 - val_accuracy: 0.1792

Epoch 00011: val_loss did not improve
Epoch 12/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.9865 - accuracy: 0.1534 - val_loss: 1.8447 - val_accuracy: 0.1769

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9629 - accuracy: 0.1776 - val_loss: 1.8109 - val_accuracy: 0.1769

Epoch 00013: val_loss did not improve
Epoch 14/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.9317 - accuracy: 0.1658 - val_loss: 1.8348 - val_accuracy: 0.1722

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9778 - accuracy: 0.1658 - val_loss: 1.8514 - val_accuracy: 0.1580

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0030 - accuracy: 0.1805 - val_loss: 1.9465 - val_accuracy: 0.1816

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9975 - accuracy: 0.1499 - val_loss: 1.8630 - val_accuracy: 0.1887

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9927 - accuracy: 0.1740 - val_loss: 1.8497 - val_accuracy: 0.1792

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9768 - accuracy: 0.1581 - val_loss: 1.9295 - val_accuracy: 0.1580

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9955 - accuracy: 0.1676 - val_loss: 1.8860 - val_accuracy: 0.1816

Epoch 00020: val_loss did not improve
Epoch 21/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0040 - accuracy: 0.1782 - val_loss: 1.8776 - val_accuracy: 0.1792

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0060 - accuracy: 0.1817 - val_loss: 1.8559 - val_accuracy: 0.1439

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.9913 - accuracy: 0.1681 - val_loss: 1.9012 - val_accuracy: 0.1840

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0132 - accuracy: 0.1699 - val_loss: 1.8319 - val_accuracy: 0.1840

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.9989 - accuracy: 0.1758 - val_loss: 1.9707 - val_accuracy: 0.1651

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0022 - accuracy: 0.1676 - val_loss: 1.7942 - val_accuracy: 0.1533

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9899 - accuracy: 0.1611 - val_loss: 1.8937 - val_accuracy: 0.1392

Epoch 00027: val_loss did not improve
Epoch 28/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.9784 - accuracy: 0.1811 - val_loss: 1.8012 - val_accuracy: 0.1816

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0165 - accuracy: 0.1687 - val_loss: 1.8468 - val_accuracy: 0.1792

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0192 - accuracy: 0.1487 - val_loss: 1.9313 - val_accuracy: 0.1580

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0188 - accuracy: 0.1528 - val_loss: 1.8226 - val_accuracy: 0.1769

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0111 - accuracy: 0.1711 - val_loss: 1.9613 - val_accuracy: 0.1462

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0326 - accuracy: 0.1664 - val_loss: 1.9267 - val_accuracy: 0.1816

Epoch 00033: val_loss did not improve
Epoch 34/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0912 - accuracy: 0.1475 - val_loss: 1.8996 - val_accuracy: 0.1816

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0071 - accuracy: 0.1670 - val_loss: 1.8723 - val_accuracy: 0.1840

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0127 - accuracy: 0.1664 - val_loss: 1.8635 - val_accuracy: 0.1840

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0046 - accuracy: 0.1735 - val_loss: 1.8665 - val_accuracy: 0.1604

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0218 - accuracy: 0.1658 - val_loss: 1.9862 - val_accuracy: 0.1627

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0192 - accuracy: 0.1593 - val_loss: 1.8266 - val_accuracy: 0.1816

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0003 - accuracy: 0.1640 - val_loss: 1.9089 - val_accuracy: 0.1816

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 153s 90ms/step - loss: 2.0419 - accuracy: 0.1617 - val_loss: 1.9906 - val_accuracy: 0.1863

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0442 - accuracy: 0.1770 - val_loss: 1.8581 - val_accuracy: 0.1840

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0040 - accuracy: 0.1617 - val_loss: 1.8592 - val_accuracy: 0.1392

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0252 - accuracy: 0.1711 - val_loss: 1.8818 - val_accuracy: 0.1580

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0436 - accuracy: 0.1504 - val_loss: 1.8239 - val_accuracy: 0.1769

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0524 - accuracy: 0.1628 - val_loss: 1.9954 - val_accuracy: 0.1392

Epoch 00046: val_loss did not improve
Epoch 47/50
1695/1695 [==============================] - 154s 91ms/step - loss: 2.0327 - accuracy: 0.1740 - val_loss: 1.8818 - val_accuracy: 0.1840

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0278 - accuracy: 0.1558 - val_loss: 1.8146 - val_accuracy: 0.1887

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 152s 90ms/step - loss: 2.0166 - accuracy: 0.1664 - val_loss: 1.9254 - val_accuracy: 0.1722

Epoch 00049: val_loss did not improve
Epoch 50/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.9985 - accuracy: 0.1853 - val_loss: 1.8423 - val_accuracy: 0.1840

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1073_ITS_ANG_XX.wav.npz           Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1051_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1037_MTI_DIS_XX.wav.npz           Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1086_TIE_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1007_IEO_FEA_MD.wav.npz           Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1037_ITS_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1070_DFA_HAP_XX.wav.npz           Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1088_TSI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1037_DFA_NEU_XX.wav.npz           Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1042_TSI_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1033_ITS_SAD_XX.wav.npz           Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1079_MTI_SAD_XX.wav.npz                  
Adam  gives the following results:
Dev set loss: 1.9123486262515075
Dev set accuracy: 0.15037593245506287
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  2126 , going to load total_load =  2120
total files =  2126 , going to load total_load =  2120
   get_sample_dimensions: 1068_DFA_ANG_XX.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/345: Preprocessed/Train/ANG/1013_TAI_ANG_XX.wav.npz                 Loading class 1/6: 'ANG', File 101/345: Preprocessed/Train/ANG/1071_IOM_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 201/345: Preprocessed/Train/ANG/1054_ITH_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 301/345: Preprocessed/Train/ANG/1090_DFA_ANG_XX.wav.npz               Loading class 1/6: 'ANG', File 345/345: Preprocessed/Train/ANG/1021_ITS_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/374: Preprocessed/Train/DIS/1028_ITH_DIS_XX.wav.npz                 Loading class 2/6: 'DIS', File 101/374: Preprocessed/Train/DIS/1083_TAI_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 201/374: Preprocessed/Train/DIS/1056_ITS_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 301/374: Preprocessed/Train/DIS/1064_TIE_DIS_XX.wav.npz               Loading class 2/6: 'DIS', File 374/374: Preprocessed/Train/DIS/1057_IOM_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/343: Preprocessed/Train/FEA/1008_TAI_FEA_XX.wav.npz                 Loading class 3/6: 'FEA', File 101/343: Preprocessed/Train/FEA/1090_ITH_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 201/343: Preprocessed/Train/FEA/1087_TIE_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 301/343: Preprocessed/Train/FEA/1078_IWL_FEA_XX.wav.npz               Loading class 3/6: 'FEA', File 343/343: Preprocessed/Train/FEA/1049_ITS_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/363: Preprocessed/Train/HAP/1086_ITH_HAP_XX.wav.npz                 Loading class 4/6: 'HAP', File 101/363: Preprocessed/Train/HAP/1008_IWL_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 201/363: Preprocessed/Train/HAP/1089_DFA_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 301/363: Preprocessed/Train/HAP/1066_TAI_HAP_XX.wav.npz               Loading class 4/6: 'HAP', File 363/363: Preprocessed/Train/HAP/1086_IOM_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/328: Preprocessed/Train/NEU/1070_TAI_NEU_XX.wav.npz                 Loading class 5/6: 'NEU', File 101/328: Preprocessed/Train/NEU/1006_TAI_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 201/328: Preprocessed/Train/NEU/1028_IWW_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 301/328: Preprocessed/Train/NEU/1028_WSI_NEU_XX.wav.npz               Loading class 5/6: 'NEU', File 328/328: Preprocessed/Train/NEU/1049_IOM_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/373: Preprocessed/Train/SAD/1052_TAI_SAD_XX.wav.npz                 Loading class 6/6: 'SAD', File 101/373: Preprocessed/Train/SAD/1071_WSI_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 201/373: Preprocessed/Train/SAD/1084_WSI_SAD_XX.wav.npz               Loading class 6/6: 'SAD', File 301/373: Preprocessed/Train/SAD/1032_DFA_SAD_XX.wav.npz                  
 MyCNN_Keras2: X_shape =  (2120, 96, 420, 1) , channels =  1
WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Looking for previous weights...
No weights file detected, so starting from scratch.
 Available GPUs =  [] , count =  0
Summary of serial model (duplicated across 0 GPUs):
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Input (Conv2D)               (None, 96, 420, 32)       320       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 48, 210, 32)       0         
_________________________________________________________________
activation_6 (Activation)    (None, 48, 210, 32)       0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 48, 210, 32)       128       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 48, 210, 32)       9248      
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 24, 105, 32)       0         
_________________________________________________________________
activation_7 (Activation)    (None, 24, 105, 32)       0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 24, 105, 32)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 24, 105, 32)       9248      
_________________________________________________________________
max_pooling2d_7 (MaxPooling2 (None, 12, 52, 32)        0         
_________________________________________________________________
activation_8 (Activation)    (None, 12, 52, 32)        0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 12, 52, 32)        0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 12, 52, 32)        9248      
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 6, 26, 32)         0         
_________________________________________________________________
activation_9 (Activation)    (None, 6, 26, 32)         0         
_________________________________________________________________
dropout_7 (Dropout)          (None, 6, 26, 32)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 4992)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 128)               639104    
_________________________________________________________________
activation_10 (Activation)   (None, 128)               0         
_________________________________________________________________
dropout_8 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 774       
_________________________________________________________________
Output (Activation)          (None, 6)                 0         
=================================================================
Total params: 668,070
Trainable params: 668,006
Non-trainable params: 64
_________________________________________________________________
Train on 1695 samples, validate on 424 samples
Epoch 1/50
1695/1695 [==============================] - 158s 93ms/step - loss: 4.0628 - accuracy: 0.2018 - val_loss: 2.0936 - val_accuracy: 0.3184

Epoch 00001: val_loss improved from inf to 2.09357, saving model to Adam0.0001weights.hdf5
Epoch 2/50
1695/1695 [==============================] - 154s 91ms/step - loss: 2.5394 - accuracy: 0.2330 - val_loss: 1.8584 - val_accuracy: 0.2618

Epoch 00002: val_loss improved from 2.09357 to 1.85844, saving model to Adam0.0001weights.hdf5
Epoch 3/50
1695/1695 [==============================] - 155s 92ms/step - loss: 2.3240 - accuracy: 0.2501 - val_loss: 1.7402 - val_accuracy: 0.2736

Epoch 00003: val_loss improved from 1.85844 to 1.74020, saving model to Adam0.0001weights.hdf5
Epoch 4/50
1695/1695 [==============================] - 156s 92ms/step - loss: 2.2561 - accuracy: 0.2560 - val_loss: 1.6207 - val_accuracy: 0.3090

Epoch 00004: val_loss improved from 1.74020 to 1.62073, saving model to Adam0.0001weights.hdf5
Epoch 5/50
1695/1695 [==============================] - 156s 92ms/step - loss: 2.1759 - accuracy: 0.2614 - val_loss: 1.6213 - val_accuracy: 0.3042

Epoch 00005: val_loss did not improve
Epoch 6/50
1695/1695 [==============================] - 157s 92ms/step - loss: 2.0164 - accuracy: 0.2873 - val_loss: 1.7026 - val_accuracy: 0.2665

Epoch 00006: val_loss did not improve
Epoch 7/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.9354 - accuracy: 0.2979 - val_loss: 1.6460 - val_accuracy: 0.3184

Epoch 00007: val_loss did not improve
Epoch 8/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.8902 - accuracy: 0.3062 - val_loss: 1.6553 - val_accuracy: 0.2429

Epoch 00008: val_loss did not improve
Epoch 9/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.8461 - accuracy: 0.3074 - val_loss: 1.6623 - val_accuracy: 0.3019

Epoch 00009: val_loss did not improve
Epoch 10/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7818 - accuracy: 0.3139 - val_loss: 1.7124 - val_accuracy: 0.2500

Epoch 00010: val_loss did not improve
Epoch 11/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7651 - accuracy: 0.3091 - val_loss: 1.6107 - val_accuracy: 0.3019

Epoch 00011: val_loss improved from 1.62073 to 1.61073, saving model to Adam0.0001weights.hdf5
Epoch 12/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.7407 - accuracy: 0.3245 - val_loss: 1.6393 - val_accuracy: 0.2453

Epoch 00012: val_loss did not improve
Epoch 13/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7102 - accuracy: 0.3168 - val_loss: 1.5861 - val_accuracy: 0.2712

Epoch 00013: val_loss improved from 1.61073 to 1.58610, saving model to Adam0.0001weights.hdf5
Epoch 14/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.7025 - accuracy: 0.3209 - val_loss: 1.6465 - val_accuracy: 0.2759

Epoch 00014: val_loss did not improve
Epoch 15/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.6290 - accuracy: 0.3528 - val_loss: 1.5955 - val_accuracy: 0.2830

Epoch 00015: val_loss did not improve
Epoch 16/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6521 - accuracy: 0.3263 - val_loss: 1.6096 - val_accuracy: 0.2901

Epoch 00016: val_loss did not improve
Epoch 17/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6164 - accuracy: 0.3528 - val_loss: 1.6153 - val_accuracy: 0.2571

Epoch 00017: val_loss did not improve
Epoch 18/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.6084 - accuracy: 0.3504 - val_loss: 1.5926 - val_accuracy: 0.2712

Epoch 00018: val_loss did not improve
Epoch 19/50
1695/1695 [==============================] - 155s 91ms/step - loss: 1.5743 - accuracy: 0.3581 - val_loss: 1.6700 - val_accuracy: 0.2642

Epoch 00019: val_loss did not improve
Epoch 20/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.5767 - accuracy: 0.3729 - val_loss: 1.5099 - val_accuracy: 0.3231

Epoch 00020: val_loss improved from 1.58610 to 1.50992, saving model to Adam0.0001weights.hdf5
Epoch 21/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.5687 - accuracy: 0.3670 - val_loss: 1.6351 - val_accuracy: 0.2736

Epoch 00021: val_loss did not improve
Epoch 22/50
1695/1695 [==============================] - 10677s 6s/step - loss: 1.5587 - accuracy: 0.3581 - val_loss: 1.6176 - val_accuracy: 0.2642

Epoch 00022: val_loss did not improve
Epoch 23/50
1695/1695 [==============================] - 208s 122ms/step - loss: 1.5811 - accuracy: 0.3699 - val_loss: 1.5741 - val_accuracy: 0.2736

Epoch 00023: val_loss did not improve
Epoch 24/50
1695/1695 [==============================] - 191s 113ms/step - loss: 1.5444 - accuracy: 0.3676 - val_loss: 1.5979 - val_accuracy: 0.2807

Epoch 00024: val_loss did not improve
Epoch 25/50
1695/1695 [==============================] - 189s 112ms/step - loss: 1.5397 - accuracy: 0.3758 - val_loss: 1.5635 - val_accuracy: 0.2948

Epoch 00025: val_loss did not improve
Epoch 26/50
1695/1695 [==============================] - 207s 122ms/step - loss: 1.5303 - accuracy: 0.3752 - val_loss: 1.5172 - val_accuracy: 0.3373

Epoch 00026: val_loss did not improve
Epoch 27/50
1695/1695 [==============================] - 172s 101ms/step - loss: 1.4852 - accuracy: 0.4206 - val_loss: 1.5097 - val_accuracy: 0.3160

Epoch 00027: val_loss improved from 1.50992 to 1.50965, saving model to Adam0.0001weights.hdf5
Epoch 28/50
1695/1695 [==============================] - 185s 109ms/step - loss: 1.5185 - accuracy: 0.3858 - val_loss: 1.5347 - val_accuracy: 0.3113

Epoch 00028: val_loss did not improve
Epoch 29/50
1695/1695 [==============================] - 171s 101ms/step - loss: 1.4767 - accuracy: 0.4018 - val_loss: 1.5319 - val_accuracy: 0.3208

Epoch 00029: val_loss did not improve
Epoch 30/50
1695/1695 [==============================] - 168s 99ms/step - loss: 1.5084 - accuracy: 0.3864 - val_loss: 1.5724 - val_accuracy: 0.3019

Epoch 00030: val_loss did not improve
Epoch 31/50
1695/1695 [==============================] - 174s 103ms/step - loss: 1.4819 - accuracy: 0.4012 - val_loss: 1.5273 - val_accuracy: 0.3113

Epoch 00031: val_loss did not improve
Epoch 32/50
1695/1695 [==============================] - 169s 100ms/step - loss: 1.5143 - accuracy: 0.3917 - val_loss: 1.5116 - val_accuracy: 0.3184

Epoch 00032: val_loss did not improve
Epoch 33/50
1695/1695 [==============================] - 181s 107ms/step - loss: 1.4648 - accuracy: 0.4124 - val_loss: 1.4429 - val_accuracy: 0.3750

Epoch 00033: val_loss improved from 1.50965 to 1.44286, saving model to Adam0.0001weights.hdf5
Epoch 34/50
1695/1695 [==============================] - 173s 102ms/step - loss: 1.4658 - accuracy: 0.3976 - val_loss: 1.4917 - val_accuracy: 0.3491

Epoch 00034: val_loss did not improve
Epoch 35/50
1695/1695 [==============================] - 179s 105ms/step - loss: 1.4612 - accuracy: 0.4065 - val_loss: 1.5344 - val_accuracy: 0.3019

Epoch 00035: val_loss did not improve
Epoch 36/50
1695/1695 [==============================] - 192s 113ms/step - loss: 1.4362 - accuracy: 0.4112 - val_loss: 1.5279 - val_accuracy: 0.3231

Epoch 00036: val_loss did not improve
Epoch 37/50
1695/1695 [==============================] - 187s 110ms/step - loss: 1.4286 - accuracy: 0.4106 - val_loss: 1.5082 - val_accuracy: 0.3160

Epoch 00037: val_loss did not improve
Epoch 38/50
1695/1695 [==============================] - 178s 105ms/step - loss: 1.4233 - accuracy: 0.4189 - val_loss: 1.5033 - val_accuracy: 0.3278

Epoch 00038: val_loss did not improve
Epoch 39/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.4270 - accuracy: 0.4295 - val_loss: 1.4820 - val_accuracy: 0.3278

Epoch 00039: val_loss did not improve
Epoch 40/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4399 - accuracy: 0.4324 - val_loss: 1.4483 - val_accuracy: 0.3491

Epoch 00040: val_loss did not improve
Epoch 41/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4120 - accuracy: 0.4271 - val_loss: 1.4991 - val_accuracy: 0.3349

Epoch 00041: val_loss did not improve
Epoch 42/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4352 - accuracy: 0.4147 - val_loss: 1.5636 - val_accuracy: 0.2948

Epoch 00042: val_loss did not improve
Epoch 43/50
1695/1695 [==============================] - 156s 92ms/step - loss: 1.4198 - accuracy: 0.4147 - val_loss: 1.5267 - val_accuracy: 0.3231

Epoch 00043: val_loss did not improve
Epoch 44/50
1695/1695 [==============================] - 155s 92ms/step - loss: 1.3893 - accuracy: 0.4342 - val_loss: 1.5112 - val_accuracy: 0.3184

Epoch 00044: val_loss did not improve
Epoch 45/50
1695/1695 [==============================] - 157s 93ms/step - loss: 1.4057 - accuracy: 0.4283 - val_loss: 1.4690 - val_accuracy: 0.3420

Epoch 00045: val_loss did not improve
Epoch 46/50
1695/1695 [==============================] - 161s 95ms/step - loss: 1.3819 - accuracy: 0.4608 - val_loss: 1.4366 - val_accuracy: 0.3679

Epoch 00046: val_loss improved from 1.44286 to 1.43663, saving model to Adam0.0001weights.hdf5
Epoch 47/50
1695/1695 [==============================] - 158s 93ms/step - loss: 1.3910 - accuracy: 0.4389 - val_loss: 1.4685 - val_accuracy: 0.3491

Epoch 00047: val_loss did not improve
Epoch 48/50
1695/1695 [==============================] - 153s 90ms/step - loss: 1.4125 - accuracy: 0.4354 - val_loss: 1.4503 - val_accuracy: 0.3608

Epoch 00048: val_loss did not improve
Epoch 49/50
1695/1695 [==============================] - 152s 90ms/step - loss: 1.3891 - accuracy: 0.4313 - val_loss: 1.4164 - val_accuracy: 0.3774

Epoch 00049: val_loss improved from 1.43663 to 1.41637, saving model to Adam0.0001weights.hdf5
Epoch 50/50
1695/1695 [==============================] - 151s 89ms/step - loss: 1.3422 - accuracy: 0.4749 - val_loss: 1.4317 - val_accuracy: 0.3726

Epoch 00050: val_loss did not improve
class_names =  ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']
       total files =  266 , going to load total_load =  266
total files =  266 , going to load total_load =  266
   get_sample_dimensions: 1075_IEO_ANG_HI.wav.npz: melgram.shape =  (1, 96, 420, 1)
 melgram dimensions:  (1, 96, 420, 1)

 Loading class 1/6: 'ANG', File 1/44: Preprocessed/Train/../Dev/ANG/1078_MTI_ANG_XX.wav.npz           Loading class 1/6: 'ANG', File 44/44: Preprocessed/Train/../Dev/ANG/1039_TIE_ANG_XX.wav.npz                  
 Loading class 2/6: 'DIS', File 1/42: Preprocessed/Train/../Dev/DIS/1062_IEO_DIS_HI.wav.npz           Loading class 2/6: 'DIS', File 42/42: Preprocessed/Train/../Dev/DIS/1044_IWL_DIS_XX.wav.npz                  
 Loading class 3/6: 'FEA', File 1/62: Preprocessed/Train/../Dev/FEA/1004_ITH_FEA_XX.wav.npz           Loading class 3/6: 'FEA', File 62/62: Preprocessed/Train/../Dev/FEA/1055_IWL_FEA_XX.wav.npz                  
 Loading class 4/6: 'HAP', File 1/47: Preprocessed/Train/../Dev/HAP/1030_ITS_HAP_XX.wav.npz           Loading class 4/6: 'HAP', File 47/47: Preprocessed/Train/../Dev/HAP/1033_MTI_HAP_XX.wav.npz                  
 Loading class 5/6: 'NEU', File 1/31: Preprocessed/Train/../Dev/NEU/1024_WSI_NEU_XX.wav.npz           Loading class 5/6: 'NEU', File 31/31: Preprocessed/Train/../Dev/NEU/1055_IWL_NEU_XX.wav.npz                  
 Loading class 6/6: 'SAD', File 1/40: Preprocessed/Train/../Dev/SAD/1019_MTI_SAD_XX.wav.npz           Loading class 6/6: 'SAD', File 40/40: Preprocessed/Train/../Dev/SAD/1056_IEO_SAD_LO.wav.npz                  
Adam  gives the following results:
Dev set loss: 1.4598447380209327
Dev set accuracy: 0.3684210479259491
(tensorflow) DN0a241607:panotti evangelie$ 
